# Phase 3設計における報酬と目的の問題：議論の記録 要約

**出典**: phase3_reward_discussion.pdf (ユーザー提供)

## 1. 背景

プロジェクトの最終的なゴールは「未知の形状に対してアフォーダンスを推測できるエージェント」を開発すること。

### これまでの経緯

- **Phase 1**: F/G（形状再構成モデル）の事前訓練を実施。基本3形状（立方体、球、円柱）の再構成学習を完了。
- **Phase 2**: Agent C（行動選択エージェント）の注意選択と比較ηのメカニズムを実装。しかし、批判的分析により設計上の問題が明らかになった：
  - 精度計算（Accuracy）のロジックに誤りがあった
  - 報酬設計が単調であり、エージェントがどの部分に注目しても正の報酬を得られるため、有意義な学習が進まなかった
  - 状態遷移の概念が欠如しており、next_stateが常に現在のstateと同じになっていた
- **Phase 2からの教訓**: 比較ηのコンセプト自体は有望であるものの、実装には改善が必要。具体的には、価値関数の出力を単一のスカラー値からη軌跡の予測へと変更し、Agent CにRNN（再帰型ニューラルネットワーク）を導入して時系列情報を扱えるようにする必要がある。また、TD誤差を軌跡パターン間の差分として再定義することが有効である。

## 2. 理論的基盤の確認

value_function_analysis_v3で展開された理論は、本プロジェクトの根幹をなすもの。主要な主張は以下の通り：

- **F/GとAgent Cの役割分担**: F/Gは「川床」に例えられ、損失関数に基づいてゆっくりと進化する、用意された世界のモデルとして機能する。一方、Agent Cは「水の流れ」に例えられ、価値関数に基づいて速い時間スケールで行動する。
- **随伴の成立**: 随伴（Adjunction）は、このF/GとAgent Cの相互作用を通じて成立する。
- **行動空間の拡張**: Agent Cの行動空間には、「何をするか」だけでなく「何を見るか」という注意の選択も含まれるべきである。
- **価値関数と目的**: 価値関数は単なる報酬の期待値ではなく、軌跡の要約として機能する。η軌跡を改善すること自体がエージェントの駆動力となり、目的は特定の軌跡パターンとして創発的に生まれると期待される。

## 3. Phase 3の設計提案と問題点

ユーザーによって提案されたPhase 3の実装計画は、PyBulletを用いた物理シミュレーション環境を舞台としている。

### 3.1. 実装計画の概要

1. **環境構築**: PyBulletでテーブル上にオブジェクトを配置し、KUKAロボットアームを設置する。
2. **アーム制御**: ロボットアームを制御するインターフェースを実装する。
3. **タスク定義**: Pickingタスク（オブジェクトを掴んで持ち上げる）を定義する。
4. **Agent Cの再設計**: DDPGやSACといった深層強化学習アルゴリズムを用いてAgent Cを再設計する。
5. **訓練ループ**: 上記の要素を統合し、訓練ループを実装する。

この計画における報酬は、以下のように定義されている：

- **タスク成功**: +1.0
- **掴み失敗**: -0.1
- **タイムステップペナルティ**: -0.01

### 3.2. 問題点

この計画には、プロジェクトの根幹をなす理論的基盤との間にいくつかの矛盾点が指摘された。

#### 問題点1：外発的報酬の使用

計画で定義された報酬（+1.0など）は、タスクの成功という外部基準によって与えられる**外発的報酬**である。これは、Agent Cの駆動力は「η軌跡の改善」という**内発的報酬**であり、目的は創発的に生まれるべきだ、というこれまでの設計思想と矛盾する。外発的報酬は、設計者が明示的に目的を与えることに他ならない。

#### 問題点2：F/Gとの接続の欠落

計画では、PyBulletから取得した点群データをAgent Cに渡すところまでは記述されているが、その点群をF/Gに通してηを計算し、その結果をAgent Cの報酬や状態の一部としてフィードバックするパイプラインが明確に示されていない。これでは、F/GとAgent Cの相互作用、すなわち随伴が成立しない。

#### 問題点3：Pickingタスクの単純さ

Pickingタスクは「掴んで持ち上げる」という単一の行動で完結するため、アフォーダンスの推測能力を検証する上で単純すぎる。アフォーダンスの真価が問われるのは、「同じオブジェクトに対して複数の異なる関わり方が可能」な状況、例えば「開ける」「押す」「回す」といった多様なインタラクションが考えられる場合である。

## 4. 核心的な問い：報酬と目的の関係

これらの問題点をきっかけに、報酬と目的の関係性について、より本質的な議論が生まれた。

ユーザーの問い：「エージェントがηを最小にする方に向かっていくなら、袋の全体形状をみてそのηが最小になるような動作を瞬時に見つけるはずだけど、考え直したけど、僕のもともとの主張はエージェントの目的を通した随伴だから、エージェントの目的を設定するのがタスク報酬だったかもしれないし、エージェントは結局報酬でしか動かないから、他の仕組みでエージェントの目的を表現するのは難しいのかな？」

この問いは、プロジェクトの核心に迫る重要な論点を含んでいる。

### 4.1. ηの最小化だけでは「目的」にならない

- ηの最小化は、エージェントが対象の形状をより良く理解しようとする「知的好奇心」に近い内発的な動機付けである。
- しかし、好奇心だけでは「袋に物を入れる」といった具体的な目的指向の行動は生まれない。
- ηが低下するのは、あくまで「袋の形状を正確に再構成できるようになった」ときであり、「袋を使って何かを成し遂げた」ときではありません。エージェントが袋を見て、触り、変形させることでηが下がったとしても、それは「袋を理解した」ことを意味するだけで、「袋を使った」ことにはならない。

### 4.2. 随伴の成立には目的が必要

- ユーザーの元々の主張通り、随伴はエージェントの「目的」を通じて初めて成立する。
- 目的が存在しなければ、F/Gは単なる形状の再構成装置に留まり、その学習結果がエージェントの行動に意味のある影響を与える（=随伴する）ことはありません。
- 「袋に物を入れる」という行動が生まれるためには、エージェントに「物を入れたい」という目的が必要である。そして、その目的はηの最小化というプロセスからは直接生まれない。

### 4.3. タスク報酬は目的の正当な表現手段である

- エージェントの目的を実装レベルで表現する上で、タスク報酬は最も直接的で正当な方法である。
- 「オブジェクトを持ち上げたら+1の報酬を与える」という設定は、「持ち上げたい」という目的をエージェントに与えることに等しい。
- したがって、タスク報酬を用いることは、理論からの逸脱や妥協ではなく、「目的を通した随伴」というコンセプトを実装に落とし込むための現実的なアプローチと言える。

### 4.4. タスク報酬に伴うジレンマ

- しかし、タスク報酬を用いることには問題も伴う。特に「未知の形状に対するアフォーダンスの推測」という最終目標の検証が困難になる。
- 「持ち上げたら+1」という明確な報酬を与えられると、エージェントはアフォーダンスを真に推測しなくても、ランダムな試行錯誤の末に偶然成功した行動を強化することでタスクを達成できてしまう。
- ここに、「タスク報酬なしでは目的が生まれず随伴が成立しない」が、「タスク報酬を与えるとアフォーダンスの検証ができない」というジレンマが生じる。

### 4.5. 可能な解決策

このジレンマを解消するため、いくつかの解決策が考えられる。

1. **タスク報酬の抽象度を上げる**: 「持ち上げる」のような具体的な指示ではなく、「このオブジェクトの状態を変化させる」といった抽象的な報酬を与える。どのような状態変化を目指すかはエージェント自身に発見させることで、より自律的な学習を促す。

2. **報酬の二重構造**: 報酬関数を、ηに基づく内発的報酬とタスク報酬の加重和 r(t) = α·Δη + β·r_task として設計する。これにより、「理解」の報酬と「目的」の報酬を両立させる。ただしこの場合、設計者がハイパーパラメータ α、β や r_task を決定する必要があり、「目的は創発する」という当初の思想からはある程度離れることになる。

## 5. 未解決の問い

この議論を通じて、Phase 3の設計、ひいてはプロジェクト全体の方向性に関わる、以下の重要な問いが未解決のまま残されている。

1. **内発的報酬の限界**: ηベースの内発的報酬だけで、エージェントは対象を「理解する」段階に留まらず、「使う」という目的指向の行動を発見することができるのでしょうか？

2. **タスク報酬の適切な抽象度**: タスク報酬を用いるとして、どのレベルの抽象度が最適なのでしょうか？具体的すぎれば試行錯誤で解けてしまい、アフォーダンスの学習につながりません。一方で、抽象的すぎると学習が収束しない可能性があります。

3. **理論と実装のギャップ**: 「目的は軌跡パターンとして創発する」という高次の理論と、「目的はタスク報酬として外部から与える」という具体的な実装との間のギャップを、どのようにすれば整合性をもって埋めることができるのでしょうか？

4. **物理シミュレーションにおけるηの役割**: 物理シミュレーション環境において、エージェントの行動がオブジェクトの形状を変化させたとき、F/Gの再構成誤差（η）はどのように変化するのでしょうか？その変化には、アフォーダンスに関する有益な情報が含まれているのでしょうか？
