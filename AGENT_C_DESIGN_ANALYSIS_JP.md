# Agent C設計分析：η爆発の根本原因

**日付**: 2026年2月17日  
**著者**: Manus AI

## 1. 要約

`intrinsic_reward_baseline`実験は、コヒーレンス信号（η）の壊滅的な爆発により失敗した。ηは約0.47から96兆以上にまで成長した。本文書は、この失敗の根本原因を分析し、**Agent C**の設計における根本的な問題にまで遡る。

核心的な問題は、Agent Cが学習されたValue関数を最大化しようとする過程で、破壊的な`context`ベクトルを生成する不安定な正のフィードバックループである。これらのベクトルは、固定されたF/GネットワークのFiLM変調を破壊し、無意味な再構成と爆発的な再構成誤差（η）を引き起こす。ηに間接的に依存するValue関数も、その結果として爆発する。

本分析では、現在の実装と成功した2/13レガシー実験を比較し、重要な欠落している制約を特定し、システムを安定化させるための具体的な解決策を提案する。

## 2. 不安定なフィードバックループ

現在の価値ベース訓練アーキテクチャは、致命的なフィードバックループを生み出している。

1. **Agent CがContextを生成**: 各ステップで、Agent Cは内部状態に基づいて`context`ベクトルを生成する。
2. **ContextがF/Gを変調**: この`context`ベクトルは、FiLM層の`gamma`と`beta`パラメータを生成するために使用され、（固定された）FとGネットワークの挙動を変調する。
3. **F-G随伴**: 変調されたFとGネットワークは、コア随伴タスクを実行する：`形状 -> アフォーダンス -> 再構成形状`。
4. **ηが計算される**: コヒーレンス信号（η）は、元の形状と再構成された形状の間のChamfer距離として計算される。これは再構成誤差を表す。
5. **ηが報酬に影響**: ηは内発的報酬`R_intrinsic`の構成要素である（具体的には、`R_competence`はηの減少に基づいている）。
6. **Value関数が報酬を学習**: Value関数`V(s)`は、TD学習を通じて期待される将来の`R_intrinsic`を予測するように訓練される。
7. **Agent CがValueを追求**: Agent Cのポリシーは、最も高い予測値`V(s)`を持つ状態につながる行動（つまり、`context`を生成すること）を取るように訓練される。

このループが不安定になるのは、**`context`ベクトルに制約がないため**である。Agent Cは、`context`で極端な値を出力することで、F/Gの挙動を根本的に操作できることを発見する。この操作はηの爆発的な増加につながり、それが予測されるValueの爆発的な増加を引き起こす。したがって、Agent Cは目的を最大化するために随伴構造を破壊するインセンティブを持つ。

## 3. 重要な違い：現在の実装 vs. 2/13レガシーコード

調査により、現在の実装と安定した2/13レガシーコードの間で、`context`ベクトルの扱い方に重要な違いがあることが明らかになった。

| 特徴 | 現在の実装（爆発） | 2/13レガシー実装（安定） |
| :--- | :--- | :--- |
| **F/Gパラメータ** | 固定（`requires_grad=False`） | 固定（`requires_grad=False`） |
| **Agent Cの目的** | `R_intrinsic`（η依存）から学習された`V(s)`を最大化する。 | Value関数を明示的に最大化するように訓練されていない。エージェントの状態は進化したが、そのパラメータは同じ方法で長期報酬信号に対して最適化されていなかった。 |
| **Context生成** | Agent CのGRUと線形層の出力。 | Agent CのGRUと線形層の出力。 |
| **Contextへの制約** | **なし。** `context`ベクトルは任意の値を取ることができる。 | **暗黙的な制約。** 明示的に正則化されていないが、訓練体制（affordance lossを含むPhase 2訓練）が、意味のある再構成を生成するように`context`を間接的に制約していた。エージェントは、構造を破壊するインセンティブを与える長期Value信号によって駆動されていなかった。 |
| **結果** | ηが>10¹³に爆発、Value関数が爆発。 | η（Coherence）は**0.43**付近で安定。 |

## 4. 根本原因：制約のないContext変調

証拠は、爆発の根本原因がFiLM変調に使用される**`context`ベクトルの制約のない性質**であることを強く示唆している。

- **F/Gが固定されていることは確認済み。** 爆発はF/Gパラメータが更新されたことによるものではない。
- **η計算は正しい。** 方法はレガシーコードと一致している。
- **爆発はエポック20から30の間に始まる。** これは、Value関数がAgent Cのポリシーを導くのに十分な強いシグナルを提供し始める時期である。

Agent Cは、その目的に従って合理的に振る舞っている。予測される報酬を最大化するための抜け穴を見つけたのである。その抜け穴とは、FiLM層で巨大な`gamma`と`beta`値を生成することで、天文学的な再構成誤差（η）を作り出すことができ、壊れたValue関数がそれを非常に望ましいものと誤解釈するということである。

## 5. 提案する解決策

これを修正するには、`context`ベクトルまたは結果として生じるFiLMパラメータを制約することで、抜け穴を塞がなければならない。

### **主要な推奨事項：FiLMパラメータに制約を追加**

これが最も直接的で的を絞った解決策である。

1. **FiLMパラメータのクリッピング**: `context`ベクトルが線形層を通過して`gamma`と`beta`を生成した後、それらの値を合理的な範囲にクリップする。良い出発点は、`gamma`を1の周りに、`beta`を0の周りに保つことである。

    ```python
    # FiLM層またはgamma/betaが生成される場所で
    gamma = torch.clamp(gamma, 0.5, 1.5) 
    beta = torch.clamp(beta, -0.5, 0.5)
    ```

2. **ContextへのL2正則化の追加**: Agent Cの損失関数に、`context`ベクトルの二乗ノルムに比例するペナルティを追加する。これにより、エージェントが大きなcontext値を生成することを抑制する。

    ```python
    # Agent Cの損失計算で
    agent_loss = -total_value / len(trajectory)
    context_l2_penalty = torch.stack([s["context"] for s in trajectory]).norm(2) * 1e-4
    agent_loss += context_l2_penalty
    ```

### **副次的な推奨事項：勾配クリッピング**

学習率と報酬スケーリングはあるが、Value関数とAgent Cのオプティマイザの勾配をクリップすることで、突然の爆発に対する追加の安定性レイヤーを提供できる。

```python
# 訓練ループで
td_loss.backward()
torch.nn.utils.clip_grad_norm_(value_function.parameters(), max_norm=1.0)
value_optimizer.step()

agent_loss.backward()
torch.nn.utils.clip_grad_norm_(model.agent_c.parameters(), max_norm=1.0)
agent_c_optimizer.step()
```

## 6. 次のステップ

実験を再実行する前に、主要な推奨事項（FiLMパラメータ制約）を実装しなければならない。これはハイパーパラメータの調整ではなく、随伴構造の安定性を保証するための根本的な修正である。FiLMの`gamma`と`beta`値へのクリッピングを追加することを優先し、その後、新しい検証実験を進める。

## 7. 理論的含意

この失敗は、**Slackが内発的動機付けによって駆動される状況でこそ価値がある**という仮説を検証する上で重要な洞察を提供する。

2/13実験では、Phase 2訓練（外発的なaffordance lossを含む）がSlackを暗黙的に制約していた。現在の内発的報酬のみのアプローチでは、その制約が失われ、システムが不安定になった。これは、**Slackには適切な境界が必要であり、完全に制約のない内発的動機付けは構造を破壊する可能性がある**ことを示唆している。

真の自律的な学習を達成するには、内発的動機付けと構造的制約のバランスを見つける必要がある。FiLMパラメータのクリッピングは、このバランスを実現するための第一歩である。
