# 価値関数は軌跡を要約する：判断形成と創発する目的（v3）— 要約

## 1. 価値関数とは何か

- 価値関数 = 「今いる状態が、将来にわたってどのような軌跡を描くか」をエージェント自身が要約する関数
- 単なる静的な評価ではなく、**未来の展開（軌跡）そのものに対する動的な予測**を内包する
- 「意味は軌跡に宿る」という中核的洞察

## 2. 二重の学習原理：川床の最適化と水の学習

### 2.1 自己と非自己で異なる学習原理

| 性質 | F/G（非自己）の学習 | Agent C（自己）の学習 |
|---|---|---|
| 学習原理 | 損失関数ベース | 価値関数ベース |
| 基準の所在 | 外部（環境の客観性） | 内部（エージェントの主観） |
| 基準の性質 | 静的・不変 | 動的・変化し続ける |
| 学習の性質 | 最適化（Optimization） | 学習（Learning） |
| アナロジー | 川床の形成（侵食） | 水の流れ方の学習 |

> 損失関数は変わらない。価値関数は変わり続ける。この違いが最適化と学習の違い。

### 2.2 二重のタイムスケール

- **速い軌跡（Agent C）**: 毎ステップ行われる速いタイムスケール
- **遅い軌跡（F/G）**: 数百から数千ステップをかけて行われる遅いタイムスケール
- 安定性と可塑性の両立

## 3. 価値関数と行動選択：注意という行動

### 3.1 価値関数が行動を導く
- エージェントはV(s')が最も高くなるような行動aを選ぶ
- 「将来の見通しが最も良い（最も望ましい軌跡を描く）状態に遷移する行動」を選択

### 3.2 行動空間の拡張：「何を見るか」の選択 ★重要★
- **Agent Cの行動空間に「何を見るか」という選択を含める**
- 物理的な操作だけでなく、「どこに注意を向けるか」という認知的な操作も含む
- フィルター機構によって実現される
- 注意選択行動もまた、価値関数によって駆動される

> 価値関数は、物理的行動だけでなく、認知的な行動（注意）をも導く。エージェントは「何をすべきか」と同時に「何を見るべきか」を学習する。

## 4. 目的の創発：軌跡パターンとしての目的

### 4.1 目的空間の設計は不要
- 唯一の駆動原理は「より良い軌跡（ηの持続的な改善）を求める」ことだけ
- 目的は事前に与えられるのではなく、**価値ある軌跡の連なりとして立ち現れてくる**

### 4.2 目的は「望ましい軌跡のパターン」として創発する
- 例：「喉が渇いている」→ 水への注意 → 認識の成立 → ηが改善 → 価値関数が学習
- この軌跡パターンこそが「水を飲む」という目的

## 5. 随伴の成立条件：F/GとAgent Cの相互作用

> 随伴の成立条件：F/Gの出力は「潜在表現」である。それが「アフォーダンス」になるのは、Agent Cがそれを行動の文脈で解釈し、選択したときである。

- F/Gだけでも、Agent Cだけでも随伴は成立しない
- **F/Gという川床の上をAgent Cという水が流れると、はじめて随伴が成立し、価値関数が評価すべき意味のある軌跡が生まれる**

## 6. 価値関数の学習：軌跡予測の自己修正

### 6.1 TD誤差は「予想した軌跡と実際の軌跡のずれ」
- δ = r(t) + γ·V(s(t+1)) - V(s(t))
- 報酬 r(t) = α·Δη（軌跡の瞬間的な傾き）

### 6.2 価値関数の学習は、軌跡予測能力の学習
- 世界のダイナミクス、すなわち軌跡の形状を予測する能力の学習

| 概念 | 軌跡的解釈 |
|---|---|
| 価値関数 V(s) | 状態sから始まる未来の軌跡の要約・予測 |
| 報酬 r(t) = α·Δη | 軌跡の瞬間的な傾き（微分） |
| TD誤差 δ | 予想した軌跡と実際の軌跡のずれ |
| 価値関数の学習 | 軌跡を予測する能力の自己修正的な洗練 |

## 7. 価値関数と高次の認知機能

- **記憶の引き出し**: ηの軌跡パターンをキーとして引き出される
- **抽象思考**: 軌跡の分解と再結合として実現される
- **休息**: 軌跡の再編成のプロセス

## 8. 結論：価値関数はAgent Cの動的な世界観である

- 価値関数 = Agent Cが経験を通じて構築する**動的な世界観**
- 目的は設計されるのではなく、価値ある軌跡のパターンとして創発する
- 価値関数が軌跡を予測し、その予測を更新し続ける限りにおいて、エージェントは自律的に世界を理解し、その中で賢明に行動することができる
