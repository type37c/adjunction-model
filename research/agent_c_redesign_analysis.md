# Agent C再設計のための統合分析

## 1. 全資料から得られた設計要件の整理

### 1.1 value_function_analysis_v3 からの要件

**最も重要な文書**。Agent Cの設計思想の根幹。

| 要件 | 内容 | 現在の実装状況 |
|---|---|---|
| 価値関数は軌跡の要約 | V(s)は「未来の軌跡の予測」であるべき | ❌ 単純なスカラー出力のみ |
| 二重の学習原理 | F/G=損失関数ベース（遅い）、Agent C=価値関数ベース（速い） | ❌ PPOで一括訓練、タイムスケールの区別なし |
| 行動空間に「注意」を含む | 「何をするか」+「何を見るか」 | ❌ 物理的行動（トルク）のみ |
| 報酬 r(t) = α·Δη | 軌跡の瞬間的な傾き | ❌ -distance のみ |
| 目的は軌跡パターンとして創発 | 事前に目的空間を設計しない | ❌ タスク報酬で明示的に目的を与えている |
| 随伴はF/GとAgent Cの相互作用で成立 | 川床（F/G）の上を水（Agent C）が流れる | ❌ F/Gは凍結、相互作用なし |

### 1.2 phase2_experiment_design からの要件

**比較η**の概念が核心。

| 要件 | 内容 | 現在の実装状況 |
|---|---|---|
| 比較η（η_whole vs η_part） | 全体と部分の再構成誤差の差を報酬に | ❌ ηを全く使っていない |
| 注意選択の行動空間 | どの部分に注目するかを選択 | ❌ 注意機構なし |
| 内発的報酬 r(t) = α·(η_whole - η_part) | 好奇心の形式的表現 | ❌ 外発的報酬のみ |
| F/Gパラメータの凍結 | Agent Cのみ学習 | ⚠️ 凍結はしているが、ηを使っていない |

### 1.3 phase3_reward_discussion からの要件

**報酬と目的のジレンマ**の整理。

| 要件 | 内容 | 現在の実装状況 |
|---|---|---|
| ηの最小化だけでは目的にならない | 好奇心≠目的指向行動 | ⚠️ 認識はしているが未解決 |
| タスク報酬は目的の正当な表現 | 理論からの逸脱ではない | ✅ タスク報酬を使用 |
| 報酬の二重構造 | r(t) = α·Δη + β·r_task | ❌ 内発的報酬が存在しない |

### 1.4 phase1_gap_analysis からの要件

| 要件 | 内容 | 現在の実装状況 |
|---|---|---|
| 「推測」する能力 | 未知形状へのアフォーダンス推測 | ❌ 未検証 |
| 能動的な情報収集 | 「どこに注目すべきか」の選択 | ❌ 注意機構なし |
| 文脈情報 | 環境やエージェントの状態に依存 | ❌ 点群のみ |

### 1.5 PURPOSE_MECHANISM_DESIGN からの要件

| 要件 | 内容 | 現在の実装状況 |
|---|---|---|
| Concern State | エージェントの関心事を表現 | ❌ 未実装 |
| Grip Monitor | ηの変化パターンを評価 | ❌ 未実装 |
| Salience Modulator | ランドスケープ→フィールド変換 | ❌ 平均プーリングのまま |
| F(x, g) | 目的条件付きアフォーダンス | ⚠️ FunctorF_v2に実装済みだが未統合 |

### 1.6 src/models/agent_c.py（オリジナル設計）からの要件

| 要件 | 内容 | Step 2の実装状況 |
|---|---|---|
| GRU + VAE構造 | RSSM（再帰的状態空間モデル） | ❌ 単純なLSTMに簡略化 |
| 潜在状態z | 再パラメータ化トリックによるサンプリング | ❌ 削除 |
| 観察はF(x)の出力 | observation_dim=16（アフォーダンス次元） | ❌ ロボット状態を直接使用 |

---

## 2. Step 2実験から得られた知見

### 2.1 失敗の原因分析

ベースラインの訓練が成功率0%、報酬-84930.71で失敗した。

**原因1: 報酬設計の根本的な問題**
- `reward = -distance` は距離が大きいと非常に大きな負の値になる
- エピソード200ステップ × 平均距離5m = 累積報酬-1000
- ロボットが暴走すると距離が数百メートルに → 報酬が-84930

**原因2: トルク制御の難しさ**
- 連続トルク制御は、初期のランダム行動では意味のある動きにならない
- 関節角度の制限がないため、ロボットが暴走する

**原因3: 状態表現の不足**
- エンドエフェクタの位置が状態に含まれていない
- オブジェクトとの相対ベクトルが含まれていない

**原因4: 理論との乖離**
- 最も根本的な問題：資料で設計された理論的枠組みを全く反映していない
- ηを使っていない、注意機構がない、比較ηがない、目的機構がない

### 2.2 Phase 1.5の成果

FunctorF_v2の訓練は成功：
- ηが行動の種類によって系統的に変化するようになった
- Static: 0.1146、Push: 0.0159、Topple: 0.0128
- 変動係数1.1247（Phase 1の24倍）

**重要な知見**: F/Gは「物理的相互作用の結果を予測する」能力を獲得している。
これは、Agent Cがηの変化を報酬として利用できる基盤が整ったことを意味する。

---

## 3. 現在のAgent Cの問題点の総括

### 根本的な問題：理論と実装の完全な乖離

現在のStep 2のAgent Cは、資料で設計された理論的枠組みを**ほぼ完全に無視**している。

1. **オリジナルのAgent C（src/models/agent_c.py）**: GRU + VAE、F(x)を観察として受け取る
2. **Step 2のAgent C**: 単純なLSTM + Actor-Critic、ロボット状態を直接使用

Step 2のAgent Cは、一般的な強化学習のAgent実装であり、このプロジェクト固有の設計思想が反映されていない。

### 具体的な欠落

1. **ηを報酬として使っていない** → 随伴の核心が欠落
2. **注意選択がない** → 「何を見るか」の学習ができない
3. **比較ηがない** → 部分と全体の関係を理解できない
4. **目的機構がない** → ランドスケープからフィールドへの変換ができない
5. **VAE構造がない** → 潜在状態の探索ができない
6. **二重のタイムスケールがない** → F/GとAgent Cの相互作用が成立しない

---

## 4. 再設計の方向性

### 4.1 段階的アプローチ

一度に全てを実装するのではなく、段階的に理論的要素を追加する。

**Stage A: 基盤の修正（即座に実装）**
- 報酬設計の修正（距離ベース → 改善ベース + スケーリング）
- 行動空間の修正（トルク制御 → 位置制御）
- 状態表現の改善（エンドエフェクタ位置、相対ベクトル）
- オリジナルAgent C（GRU + VAE）の復元

**Stage B: ηの統合（Phase 1.5の成果を活用）**
- F/Gの出力をAgent Cの観察に含める
- ηの変化を内発的報酬として追加
- 報酬の二重構造: r(t) = α·Δη + β·r_task

**Stage C: 注意機構の導入**
- Salience Modulatorの実装（平均プーリング → 注意機構）
- 行動空間に「注意選択」を追加
- 比較η（η_whole vs η_part）の実装

**Stage D: 目的機構の統合**
- Concern State、Grip Monitorの実装
- F(x, g)の目的条件付け
- end-to-end訓練

### 4.2 各Stageでの検証ポイント

| Stage | 検証ポイント | 成功基準 |
|---|---|---|
| A | ベースラインが学習するか | 成功率 > 50% |
| B | ηを使うと学習が速くなるか | 収束速度が2倍以上 |
| C | 注意選択が有意味か | 注意がオブジェクトの関連部分に集中 |
| D | 目的が創発するか | 異なるタスクで異なるConcern Stateパターン |
