# Phase 2.1 設計改訂サマリー

**日付**: 2026-02-19  
**改訂バージョン**: v2.2  
**ステータス**: 設計完了、実装待ち

## 1. 改訂の背景

ユーザーが提供した`phase3_reward_discussion.pdf`での理論的議論により、当初のPhase 2.1設計（v2.1: 軌跡予測ベース）およびPyBullet実装計画に、以下の3つの根本的な問題があることが明らかになった：

### 問題点1：外発的報酬の使用

- **問題**: タスク報酬（+1.0など）を使うと、設計者が明示的に目的を与えることになり、「目的の創発」という理論的目標と矛盾する
- **核心的な論点**: ηの最小化（内発的報酬）だけでは「理解」に留まり、「使う」という目的指向の行動は生まれない

### 問題点2：F/Gとの接続の欠落

- **問題**: PyBulletの点群をF/Gに通してηを計算し、Agent Cにフィードバックするパイプラインが不明確
- **核心的な論点**: F/GとAgent Cの相互作用（随伴）が成立しなければ、プロジェクトの理論的基盤が崩れる

### 問題点3：Pickingタスクの単純さ

- **問題**: 単一行動で完結するPickingタスクでは、アフォーダンス推測能力を真に検証できない
- **核心的な論点**: アフォーダンスの真価が問われるのは、「同じオブジェクトに対して複数の異なる関わり方が可能」な状況

## 2. 理論的議論の核心

`phase3_reward_discussion.pdf`での議論から得られた重要な洞察：

### 洞察1：随伴の成立には目的が必要

> 「目的が存在しなければ、F/Gは単なる形状の再構成装置に留まり、その学習結果がエージェントの行動に意味のある影響を与える（=随伴する）ことはありません。」

- 随伴は、エージェントの「目的」を通じて初めて成立する
- 「袋に物を入れる」という行動が生まれるには、「物を入れたい」という目的が必要

### 洞察2：タスク報酬は目的の正当な表現手段

> 「エージェントの目的を実装レベルで表現する上で、タスク報酬は最も直接的で正当な方法です。」

- タスク報酬を用いることは、理論からの逸脱や妥協ではない
- 「目的を通した随伴」というコンセプトを実装に落とし込むための現実的なアプローチ

### 洞察3：ジレンマの存在

- 「タスク報酬なしでは目的が生まれず随伴が成立しない」
- 「タスク報酬を与えるとアフォーダンスの検証ができない」（ランダムな試行錯誤でも成功できてしまう）

## 3. Phase 2.1 v2.2の解決策

改訂設計（v2.2）では、上記の理論的議論を踏まえ、以下の解決策を提案する：

### 解決策1：報酬の二重構造

```
r(t) = α · r_intrinsic(t) + β · r_task(t)
```

- **内発的報酬** `r_intrinsic(t) = α · (η_whole - η_part)`: エージェントの「理解への駆動力」
- **タスク報酬** `r_task(t)`: エージェントの「目的」
- 両者のバランスを調整することで、「理解を深めながら目的を達成する」行動を学習

**理論的正当性**: phase3_reward_discussion.pdfのセクション4.5「可能な解決策」で提案された「報酬の二重構造」に対応

### 解決策2：F/GとAgent Cの明確な統合パイプライン

各タイムステップの処理フロー：

1. 環境から点群 x_t を観測
2. F/Gで再構成誤差を計算：η_whole, η_part
3. 内発的報酬を計算：r_intrinsic(t) = α · (η_whole - η_part)
4. Agent Cが次の行動を決定（入力にF(x_t)のアフォーダンス特徴を含む）
5. 物理的行動を実行し、環境が変化
6. タスク報酬を計算：r_task(t)
7. 総報酬を計算：r(t) = α · r_intrinsic(t) + β · r_task(t)
8. Agent Cを更新

**理論的正当性**: F/Gは「川床」として世界モデルを提供し、Agent Cは「水の流れ」として価値関数に基づいて行動する。随伴は、F/Gが提供するアフォーダンス特徴と、Agent Cの目的（タスク報酬）を通じて成立する。

### 解決策3：複数インタラクションが可能なタスク

#### タスク例1：Container Filling（容器充填）

- **目標**: ボウルまたはカップに小さなオブジェクトを入れる
- **必要なアフォーダンス理解**: 開口部の位置、深さ、安定性
- **複数のインタラクション**: 容器を持ち上げる、傾ける、ボールを掴む、入れる

#### タスク例2：Articulated Object Manipulation（関節物体操作）

- **目標**: 引き出しやドアを開ける
- **必要なアフォーダンス理解**: 取っ手の位置、引く/押す方向、関節の軸
- **複数のインタラクション**: 取っ手を掴む、引く、押す、回す

**理論的正当性**: アフォーダンスの真価が問われるのは、「同じオブジェクトに対して複数の異なる関わり方が可能」な状況である（phase3_reward_discussion.pdf, セクション3.2）

### 解決策4：抽象度の高いタスク報酬

タスク報酬は「どのように達成するか」を指定せず、「何を達成するか」のみを指定：

- **成功報酬**: タスク完了時に +1.0
- **進捗報酬**: タスクに向けた有意義な進捗に対して +0.1
- **失敗ペナルティ**: -0.1
- **タイムステップペナルティ**: -0.01

**理論的正当性**: phase3_reward_discussion.pdfのセクション4.5「タスク報酬の抽象度を上げる」に対応。エージェント自身に「どのように達成するか」を発見させることで、より自律的な学習を促す。

## 4. 実装計画の概要

### 4.1. 技術スタック

- **物理シミュレーション**: PyBullet
- **ロボット**: KUKA iiwaアーム + グリッパー
- **Agent C**: LSTM-based Actor-Critic (PPO/SAC)
- **F/G**: Phase 1で事前訓練済み（`phase1_final.pt`）、パラメータ固定

### 4.2. 訓練スケジュール

1. **Phase 2.1a (1000エピソード)**: Container Fillingタスクで訓練
2. **Phase 2.1b (1000エピソード)**: Articulated Object Manipulationタスクで訓練
3. **Phase 2.1c (500エピソード)**: 未知のオブジェクトでテスト（汎化性能の検証）

### 4.3. 期待される成果

- **定量的**: タスク成功率80%以上、未知オブジェクトで50%以上
- **定性的**: F/GとAgent Cの随伴の実証、報酬の二重構造の有効性、軌跡予測の有効性
- **理論的**: 「目的を通した随伴」の具体的実装、内発的報酬と外発的報酬の統合

## 5. 未解決の問い（今後の研究課題）

Phase 2.1で部分的に検証できるが、完全には解決されない問い：

1. **内発的報酬の限界**: ηベースの内発的報酬だけで目的指向の行動が発見できるか？
   - Phase 2.1では報酬の二重構造を用いることで部分的に回避
   - 将来的には、α=1.0, β=0.0（内発的報酬のみ）での実験も検討

2. **タスク報酬の適切な抽象度**: どのレベルの抽象度が最適か？
   - Phase 2.1では抽象的な報酬を用いる
   - 異なる抽象度の報酬を比較する実験も今後検討

3. **理論と実装のギャップ**: 「目的は軌跡パターンとして創発する」という高次の理論と、「目的はタスク報酬として外部から与える」という具体的な実装との間のギャップをどう埋めるか？
   - Phase 2.1は、このギャップを埋めるための一つのアプローチを提示
   - 完全な解決には、さらなる理論的・実験的研究が必要

4. **物理シミュレーションにおけるηの役割**: エージェントの行動がオブジェクトの形状を変化させたとき、ηはどのように変化するか？
   - Phase 2.1で直接検証可能
   - ηの変化パターンがアフォーダンスに関する有益な情報を含むかを分析

## 6. まとめ

Phase 2.1 v2.2は、`phase3_reward_discussion.pdf`で提起された理論的課題を真摯に受け止め、以下の方針で設計された：

1. **PyBullet物理シミュレーション**によるアフォーダンスの物理的グラウンディング
2. **報酬の二重構造**による内発的報酬と外発的報酬の統合
3. **F/GとAgent Cの明確な統合パイプライン**による随伴の成立
4. **複数インタラクション可能なタスク**によるアフォーダンス推測能力の真の検証

この設計は、「目的を通した随伴」という理論的核心を、実装レベルで具体化する試みである。Phase 2.1の実験を通じて、プロジェクトは「内省的な観察者」から「世界と相互作用する実践者」へと進化する。

---

**次のステップ**: Phase 2.1 v2.2の実装を開始する。詳細な実装計画は `experiments/phase2.1_trajectory_prediction/DESIGN_PROPOSAL_v2.2_PYBULLET.md` を参照。
