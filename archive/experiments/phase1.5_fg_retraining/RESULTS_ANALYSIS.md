# Phase 1.5 結果分析

**実験日**: 2026年2月19日  
**実験者**: Manus AI Agent

## 実験の目的

Phase 1で訓練されたF/Gは、形状の再構成のみを学習しており、物理的相互作用に対するηの反応が鈍かった（CV = 0.0465）。Phase 1.5では、「形状 + 行動 → 結果」のデータでF/Gを再訓練し、ηが物理的相互作用に対して意味のある信号を出すようになるかを検証した。

## 実験設計

### Phase 1.5の訓練

- **データ**: PyBulletで収集した240エピソード
  - オブジェクト: box, cup, bowl
  - 行動: push, pull, lift, topple
  - 各組み合わせ: 20エピソード
- **モデル**: FunctorF_v2（近傍構造 + 目的条件付け）+ FunctorG
- **損失関数**: `L = λ_recon · L_recon + λ_action · L_action`
  - λ_recon = 1.0
  - λ_action = 0.5
- **訓練**: 10エポック、バッチサイズ16

### Step 1の再実行

- Phase 1.5で訓練されたF/Gを使ってStep 1の実験を再実行
- 各オブジェクト・行動の組み合わせで5試行
- 総試行数: 75

## 結果

### ηの統計

| 指標 | 値 |
|---|---|
| 平均 | 0.0355 |
| 標準偏差 | 0.0399 |
| 最小値 | 0.0120 |
| 最大値 | 0.1187 |
| **変動係数（CV）** | **1.1247** |

### 行動別のη

| 行動 | 平均η | 標準偏差 | 試行数 |
|---|---|---|---|
| **Static（静止）** | **0.1146** | 0.0019 | 15 |
| Lift（持ち上げ） | 0.0199 | 0.0008 | 15 |
| Push（押す） | 0.0159 | 0.0007 | 15 |
| Pull（引く） | 0.0141 | 0.0004 | 15 |
| **Topple（倒す）** | **0.0128** | 0.0005 | 15 |

### オブジェクト別のη

| オブジェクト | 平均η | 標準偏差 | 試行数 |
|---|---|---|---|
| Box | 0.0358 | 0.0402 | 25 |
| Cup | 0.0353 | 0.0403 | 25 |
| Bowl | 0.0354 | 0.0409 | 25 |

## 主要な発見

### 1. ηが行動の種類に対して大きく変化する

Phase 1では、ηは行動の種類によらずほぼ一定（0.0245 ~ 0.0346）だったが、Phase 1.5では**行動の種類によってηが9倍も変化する**（0.0128 ~ 0.1146）。

特に、**Static（静止観察）のηが他の行動と比較して約8倍高い**。これは、F/Gが「物理的相互作用が起きたかどうか」を明確に区別できるようになったことを示している。

### 2. 変動係数が24倍に増加

- **Phase 1**: CV = 0.0465
- **Phase 1.5**: CV = 1.1247
- **増加率**: 24.2倍

これは、Phase 1.5のF/Gが**物理的相互作用に対して非常に敏感に反応する**ようになったことを示している。

### 3. オブジェクトの種類による差は小さい

オブジェクトの種類（box, cup, bowl）によるηの差は小さく、平均値はほぼ同じ（0.035前後）。これは、F/Gが学習したのは「オブジェクトの形状の違い」ではなく、**「物理的相互作用のパターン」**であることを示唆している。

## 解釈

### なぜStaticのηが最も高いのか？

Phase 1.5の訓練では、「初期状態 → 行動 → 最終状態」のペアを学習した。つまり、F/Gは「この行動を実行すると、形状がこのように変化する」というパターンを学習している。

Staticの場合、**行動が実行されていないのに、F/Gは「行動に応じた変化」を予測しようとする**。その結果、予測と実際の点群の間に大きなズレが生じ、ηが高くなる。

逆に、Push/Pull/Lift/Toppleの場合、F/Gは訓練データで見たパターンに近い変化を予測できるため、ηが低くなる。

### これは「アフォーダンス」なのか？

現時点では、**まだ「アフォーダンス」とは言えない**。

理由：
1. F/Gは「行動 → 結果」のパターンを学習しているが、これは**順モデル（forward model）**であり、アフォーダンスではない
2. アフォーダンスは「このオブジェクトで何ができるか」であり、「この行動をするとどうなるか」ではない
3. 目的（goal）の条件付けはあるが、目的は外部から与えられており、エージェント自身が生成していない

ただし、**アフォーダンスへの重要な第一歩**であることは間違いない。F/Gが物理的相互作用のパターンを学習したことで、Agent Cがこの情報を使って行動を選択できる可能性が生まれた。

## Phase 2.1への影響

### 内発的報酬としての有効性

Phase 1.5のηは、**内発的報酬として使える可能性がある**。

- ηが高い = 予測と実際のズレが大きい = 「予想外の状況」
- ηが低い = 予測と実際のズレが小さい = 「予想通りの状況」

エージェントが「予想外の状況」を避けるように学習すれば、安定した行動パターンを獲得できる。逆に、「予想外の状況」を探索するように学習すれば、新しい行動パターンを発見できる。

ただし、**Staticのηが最も高い**という問題がある。このまま内発的報酬として使うと、エージェントが「何もしない」ことを学習してしまう可能性がある。

### 推奨される次のステップ

**Step 2の実装を優先すべき**：

1. Phase 1.5のF/GをAgent Cと統合する
2. タスク報酬のみで訓練し、「F/G特徴を使うAgent C」と「使わないAgent C」の学習速度を比較する
3. 差が出れば、F/Gのアフォーダンス特徴が有用であることの証拠となる
4. その時点で、内発的報酬の導入を検討する

## 結論

Phase 1.5は**大きな成功**だった。F/Gが物理的相互作用に対して敏感に反応するようになり、ηが意味のある信号を出すようになった。

ただし、これはまだ「アフォーダンス」ではなく、「順モデル」に近い。次のステップは、Agent Cとの統合を通じて、F/Gの中間表現が実際にタスク達成に有用かどうかを検証することである。

哲学的な議論から得られた洞察（目的の介入、随伴の構造）は、Phase 1.5の設計に直接反映され、実験的に検証された。理論と実装の往復が、プロジェクトを前進させている。
