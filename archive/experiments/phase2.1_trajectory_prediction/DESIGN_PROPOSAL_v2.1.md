# Phase 2.1 実験設計提案：価値関数による軌跡予測

## 1. 背景と目的

Phase 2 (v1)の実験と、その後の「価値関数設計書(v3)」の分析から、Agent Cの学習メカニズムを根本的に見直す必要性が明らかになった。Phase 2.1では、この教訓に基づき、**「価値関数は軌跡を予測する」**という理論的核心を直接的に検証する。

**目的**: Agent Cが、リカレントな状態表現と、軌跡を予測する価値関数を用いて、未知の複合形状に対する最適な注意選択方策を学習できるかを検証する。

## 2. 実験設計の主要な変更点

Phase 2 (v1)からの変更点は以下の3つに集約される。

### 2.1. 価値関数V(s)の再設計：軌跡パターンの予測

- **変更前**: `V(s)`は将来のスカラ報酬の期待値を予測する。
- **変更後**: `V(s, a)`は、状態`s`で行動`a`（注意選択）を取った場合に、その後の**Nステップで観測されるη軌跡のパターン**を予測する。出力はスカラ値ではなく、`N`次元のベクトルとなる。

### 2.2. Agent Cのアーキテクチャ変更：注意履歴の保持

- **変更前**: Agent Cの状態は、オブジェクト全体の静的な特徴量で固定。
- **変更後**: Agent Cに**LSTM (Long Short-Term Memory)**層を導入する。各ステップで、観測された部分的なアフォーダンス特徴量と、前回の隠れ状態を入力とし、現在の注意選択の文脈を保持した状態で行動を決定する。

### 2.3. 報酬とTD誤差の再定義

- **報酬**: `r(t)`は引き続き `η_whole - η_part` とする。これは軌跡の瞬間的な傾きとして有効である。
- **TD誤差**: 価値関数の学習は、**「予測した軌跡パターン」と「実際に観測された軌跡パターン」の差分**に基づいて行われる。これにより、エージェントは軌跡の形状そのものを予測する能力を洗練させていく。

## 3. 実験の仮説

この設計により、Agent Cは単に目先の報酬を最大化するのではなく、**「より持続的にηが改善する軌跡（＝より効率的に形状を理解できる軌跡）」**を予測し、そのような軌跡を生み出す注意選択行動を学習するはずである。これは、v3文書で示された「目的の創発」に向けた、具体的かつ検証可能な一歩となる。

## 4. 評価指標

- **軌跡予測誤差**: 価値関数が予測したη軌跡と、実際のη軌跡の平均二乗誤差。
- **累積報酬**: 1エピソードあたりの総報酬。より効率的な探索ができるほど高くなる。
- **探索ステップ数**: オブジェクトの全ての部分を探索し終えるまでのステップ数。
- **最終的なアフォーダンス予測精度**: （修正された`_compute_affordance_accuracy`を用いて）エピソード終了後、各部分のアフォーダンス予測がグラウンドトゥルースと一致するかを評価する。

## 5. 実装計画（概要）

1.  **Agent Cのモデル変更**: `agent_c_attention.py`にLSTM層を追加。
2.  **価値関数の出力変更**: `agent_c_attention.py`のQ-networkの最終層を、スカラ出力からN次元ベクトル出力に変更。
3.  **訓練ループの修正**: `run_phase2.py`（をコピーして`run_phase2.1.py`を作成）の訓練ループを、軌跡パターンの差分でTD誤差を計算するように修正。
4.  **評価指標の実装**: 上記の新しい評価指標を実装。

この設計により、Phase 2 (v1)の課題を克服し、プロジェクトの理論的基盤に沿った、より意味のある実験が可能となる。
