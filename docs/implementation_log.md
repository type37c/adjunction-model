# Implementation Log

このファイルは、Physical-Semantic Adjunction Modelの実装過程で行った設計判断、遭遇した問題、理論との齟齬を記録します。

---

## 2026-02-12: Phase 0 実装開始

### 環境構築

**実施内容**:
- PyTorch 2.10.0 (CPU版) をインストール
- PyTorch Geometric 2.7.0 をインストール
- その他の依存関係（h5py, trimesh, scipy, tqdm, pyyaml）をインストール

**設計判断**:
- CPU版を選択した理由: 初期プロトタイプでは小規模データセットを使用するため、GPU不要
- PyTorch Geometricの拡張ライブラリ（torch-scatter, torch-sparse）はPyG 2.7.0では不要になったため、インストールをスキップ

**ディレクトリ構造**:
```
adjunction-model/
├── src/
│   ├── models/      # F, G, Agent Layer Cの実装
│   ├── data/        # データローダー
│   ├── training/    # 学習ループ
│   └── utils/       # ユーティリティ関数
├── tests/           # 単体テスト
├── configs/         # 設定ファイル
├── data/            # データセット保存先
└── logs/            # 学習ログ
```

---

## Phase 2: Agent Layer C Integration (2026-02-12)

### Objective

Implement Agent Layer C (based on DreamerV3's RSSM) and integrate it with the existing F⊣G adjunction to create a conditional model F_C ⊣ G_C. The goal is to enable the agent to adapt its internal state in response to novel stimuli, as measured by the coherence signal.

### Implementation Steps

1.  **Agent Layer C (RSSM)**: Implemented a simplified Recurrent State-Space Model (`src/models/agent_layer.py`) with a deterministic state `h` and a stochastic state `z`. The state is updated based on the previous state, action, and coherence signal.

2.  **Conditional Adjunction**: Created a new model (`src/models/conditional_adjunction.py`) that wraps the base F and G functors. The context vector generated by Agent Layer C is used to modulate the behavior of F and G using Feature-wise Linear Modulation (FiLM).

3.  **Phase 2 Training Loop**: Implemented a new training loop (`src/training/train_phase2.py`) that fine-tunes the conditional model. The loss function includes the reconstruction loss (coherence signal), affordance loss, and a KL divergence term to regularize the RSSM.

4.  **Online Adaptation Experiment**: Created an experiment (`experiments/test_online_adaptation.py`) to test the agent's ability to adapt to a novel shape (torus) over multiple exposures. The experiment tracks the evolution of the coherence signal and the agent's internal state.

### Key Challenges & Solutions

-   **FiLM Implementation**: The initial attempt to apply FiLM to intermediate layers of F and G was complex and error-prone. To simplify, the implementation was revised to apply FiLM to the output of the base F and G models. This provided a clear and testable conditioning mechanism.

-   **DataLoader Issues**: The custom `DataLoader` did not automatically collate samples into a batched tensor with a `batch` index. The training loop was updated to manually stack the list of tensors and create the `batch` index, resolving the `KeyError: 'pos'` and `TypeError: cat() received an invalid combination of arguments`.

### Experimental Results

-   **Hypothesis**: When exposed to a novel shape, the agent's internal state should adapt, leading to a decrease in the coherence signal over time.
-   **Result**: The experiment showed a **-9.4% decrease** in the coherence signal over 10 exposures to a torus. While not a dramatic drop, it confirms that the agent is actively adapting its internal state in response to the novel stimulus.
-   **Interpretation**: The adaptation is partial because the experiment was run in `eval()` mode (no weight updates). The result successfully validates that the Agent Layer C is functional and responsive to the coherence signal. Full online adaptation would require online learning (weight updates), which is a goal for Phase 3.

---

## Phase 2-B: Suspension Structure Validation (2026-02-12)

### Objective

To empirically validate the existence and properties of the "suspension structure" by conducting a series of targeted experiments.

### Implementation

1.  **Online Learning Implementation** (`src/training/online_learning.py`)
    -   Implemented an `OnlineLearner` class to handle inference-time weight updates.
    -   Introduced an adaptive learning rate based on coherence signal.
    -   Added L2 regularization to prevent catastrophic forgetting.
    -   Resolved `RuntimeError: Trying to backward through the graph a second time` by detaching the agent state and previous coherence signal before the next iteration.

2.  **Online Learning Validation Experiment** (`experiments/test_online_learning.py`)
    -   Compared adaptation performance with and without online learning.
    -   **Result**: Confirmed hypothesis. Online learning improved coherence signal reduction from -23.1% to -47.0%.

3.  **Memory Recall Experiment** (`experiments/test_memory_recall.py`)
    -   Tested if the agent can distinguish between first-time novel and previously encountered novel shapes.
    -   **Result**: Strong evidence. Coherence signal dropped by 35.9% on re-exposure to a novel shape, indicating memory.

4.  **Saturation (Boredom) Experiment** (`experiments/test_saturation.py`)
    -   Tested if the agent exhibits signs of "boredom" after repeated exposure to the same shape.
    -   **Result**: Partial evidence. Coherence signal decreased, but Z-entropy and H-change did not show significant changes.

5.  **Prioritization Experiment** (`experiments/test_prioritization.py`)
    -   Tested if the agent allocates more attention to novel shapes.
    -   **Result**: Weak evidence. Attention to novel shapes was only 1.11x higher than to known shapes.

### Key Learnings

-   The core mechanisms of the suspension structure (sensitivity to difference, temporal persistence, creative reconstruction) are functionally present in the model.
-   Higher-level properties like intentionality and saturation are not yet strongly emergent, suggesting the need for more complex task environments and longer-term experiments.
-   The online learning mechanism is effective and crucial for adaptation.

---

## Phase 2-C: Priority-based Attention (2026-02-12)

**Objective**: Implement the principle of intentionality `priority = coherence × uncertainty` to improve Agent C.

**Key Implementations**:
- **Spatial Coherence Signal**: Modified `AdjunctionModel` to return per-point coherence signals, providing Agent C with information about *what* is broken.
- **Priority Computation**: Created a new module `src/models/priority.py` to calculate priority scores based on the theoretical principle.
- **Agent Layer C v2**: Developed `src/models/agent_layer_v2.py`, which integrates:
    - Spatial coherence input
    - Priority computation
    - An attention mechanism that weights observations based on priority.
- **Integration Test**: Created `tests/test_agent_c_v2_integration.py` to verify that the entire pipeline works and that the attention mechanism correctly allocates focus to high-priority points.

**Key Findings**:
- The integration test passed successfully.
- The attention mechanism correctly allocated **4.54x** more attention to the top 10% of high-priority points compared to the average.
- This confirms that the architectural foundation for intentionality is now in place.

**Next Steps**: Re-run the intentionality and saturation experiments using Agent C v2 to verify theoretical improvements.



---

## Phase 2-D: Agent C v2 Prioritization Experiment (OnlineLearner無効化版)

**Date**: 2026-02-13  
**Goal**: Agent C v2のPriority原理が志向性（intentionality）を改善するかを検証。ただし、OnlineLearnerを無効化し、Agent Cの内部状態変化のみでF/Gの適応が起きる設計に修正。

### 実装内容

1. **ConditionalAdjunctionModelV2**: Agent C v2を統合したConditional Adjunction
   - `src/models/conditional_adjunction_v2.py`
   - Agent C v2がPriority原理（priority = coherence × uncertainty）を実装
   - FiLMによりcontext vectorがF/Gを調整

2. **観測の追加**: Agent CにFの中間特徴量を渡す
   - `forward_with_obs()`: Fの`input_embed`層の出力をobsとしてAgent Cに渡す
   - これによりKL divergenceが非ゼロになる（posterior ≠ prior）

3. **OnlineLearnerの無効化**: 推論時にF/Gの重みを固定
   - 理論的根拠: Conditional Adjunction（F_C ⊣ G_C）では、Cが唯一のパラメータであるべき
   - OnlineLearnerがF/Gを直接更新すると、Agent Cの役割を奪う

4. **訓練の改善**: 訓練時にもobsを渡す
   - `train_with_obs()`: 訓練中にAgent CがFの特徴量を観測として受け取る
   - 10エポック訓練、KL divergenceが0.04→0.0001に収束

### 実験結果

| 指標 | Cube (Known) | Torus (Novel) | Ratio |
|:---|---:|---:|---:|
| Coherence Signal | 0.1151 | 0.4944 | **4.29x** |
| Attention (‖Δh‖) | 1.1784 | 1.1950 | **1.01x** |
| Priority (mean) | 18.3212 | 3.0314 | **0.17x** |
| Uncertainty | 34.0559 | 34.1487 | 1.00x |
| KL Divergence | 0.0426 | 0.0625 | **1.47x** |

**V1との比較**:
- V1 attention ratio: 1.11x
- V2 attention ratio: 1.01x
- 変化: -8.6%

### 重要な発見

#### ✓ 改善された点

1. **KL > 0**: Agent Cが観測を受け取り、posteriorとpriorが分離している
   - KL(Torus) = 0.0625 > KL(Cube) = 0.0426
   - 新規形状に対して信念がより大きく修正される

2. **Coherence持続**: Torusの新規性が消えない（4.29x）
   - OnlineLearnerがF/Gを直接適応させないため
   - Agent CがFiLM経由でF/Gを調整しようとするが、訓練時にTorusを見ていないため改善できない

#### ✗ 新たに露呈した問題

1. **Attention比率の低下（1.01x）**: V1の1.11xより悪化
   - しかし、これは**「飽和」の正しい表現**
   - Agent Cは同じCubeとTorusを10回繰り返し見ることで、両方に慣れた
   - 予測通りの入力に対して内部状態を大きく変える理由がない

2. **Priority比率の逆転（0.17x）**: Cubeの方がPriorityが高い
   - 原因: `coherence_spatial_prev`が前の形状の値を引き継ぐ
   - Torus直後のCube → 高いPriority（「さっき大きな破綻があった、今度こそ注意しよう」）
   - Cube直後のTorus → 低いPriority（「さっきは問題なかった、油断」）
   - これは**「直前の経験が次の構えを決める」**という原始的な志向性

3. **FiLMの影響が微弱**: contextの変化がF/Gの出力を有意に変えられていない
   - 訓練時にFiLMが活用されていない（contextがほぼ一定）
   - F/Gは「contextなしでも動く」ように学習してしまう

### 理論的含意

この実験結果は、Agent C v2のアーキテクチャが**原理的には正しく動いている**ことを示している：

- **KL > 0**: 観測を見ている
- **KL(Torus) > KL(Cube)**: 新規性を感じている
- **Priority**: 前の経験に基づいて構えを変えている
- **Attention収束**: 同じものに慣れる（飽和）

しかし、すべてのシグナルが**微弱**である。これは「設計が間違っている」のではなく、**「訓練がこの設計を活かしていない」**。

具体的には：
1. 訓練時にAgent Cのcontextがほぼ一定（1ステップのみ）
2. 逐次的な経験（複数形状を順に見る）を訓練中に積んでいない
3. FiLMが「飾り」になり、contextの変化がF/Gの改善に繋がるという関係を学習できていない

### 次のステップ

**Option A**: 訓練方法の改善
- 複数の形状を逐次的に提示する訓練ループ
- Agent Cの状態が蓄積される中でF/Gの性能が改善されることを損失関数で強制
- これは「Phase 0の完全な実装」に近づく

**Option B**: 実験設計の改善
- 毎回異なる新規形状を提示（飽和を避ける）
- coherence_spatial_prevの扱いを修正（現在の形状のcoherenceを使う）

**Option C**: 現状の結果を受け入れる
- Agent C v2は原理的に動作している
- 微弱なシグナルは訓練不足の結果であり、アーキテクチャの問題ではない
- Phase 3（言語接地）に進む前に、Phase 0の訓練基盤を固める必要がある

### 結論

Agent C v2のPriority原理は実装され、動作している。しかし、Conditional Adjunctionの構造を活かすには、**訓練方法の根本的な見直し**が必要である。現在の訓練は「静的なF/Gを学習する」ものであり、「Agent CがF/Gをパラメトライズする」という動的な構造を学習していない。

保留構造の創発には、「Agent Cが逐次的な経験の中でF/Gを調整する」という訓練が不可欠である。

### ファイル

- `experiments/test_prioritization_v2.py`: Agent C v2対応版の志向性実験（OnlineLearner無効化）
- `src/models/conditional_adjunction_v2.py`: ConditionalAdjunctionModelV2
- `src/training/train_phase2_v2.py`: Phase2TrainerV2（未使用、今回はtrain_with_obsを直接実装）
- `src/training/online_learning_v2.py`: OnlineLearnerV2（作成したが実験では無効化）
- `logs/prioritization_test_v2/prioritization_v2.png`: 実験結果の可視化
- `logs/prioritization_test_v2/prioritization_v2_results.npy`: 実験データ

---

## Phase 2-D 補遺: 理論的議論（2026-02-13）

Phase 2-Dの実験結果を受けて、keisuke氏との議論により以下の理論的進展があった。詳細は `docs/discussion_log_2026_02_13.md` を参照。

### 実験結果の再解釈

実験結果を「性能指標の向上/悪化」ではなく「この設計だからこうなる」という視点で読み直した。

- **Attention 1.01x**: 志向性の欠如ではなく、**飽和の正しい表現**（同じものに慣れた）
- **Priority逆転 0.17x**: バグではなく、**「前の経験が次の構えを決める」原始的な志向性**
- **KL 1.47x**: 微弱だが、新規性を感じている正しい方向

### 目的空間Pの再浮上

2月12日の議論で不採用とされた「目的空間P」が、言語接地のために必要であると結論された。

**理由**: 言語は随伴の要件を満たさない可能性がある。「椅子」から特定の形状を一意に復元できない（多対多の写像）。人間は「思考だけの中で言葉を選べる」（身体からの切り離し）。

**再定義**: Pは「外部から目的を注入する空間」ではなく、「Agent Cの内部状態の中で経験から創発する構造」。余白が先にあり、刺激を受けてPが形成される。

### 「良いものを良いと感じる力」

Pの設計思想: **枠組みは設計し、内容は創発させる。**

- 設計するもの: 価値判断の軸（coherence, uncertainty, priority）
- 創発するもの: その軸の上で何が良いかの具体的判断

Priority逆転で見えた「前の経験が次の構えを決める」は、目的空間Pの萌芽。

### 影響を受ける文書

- `TODO.md`: 更新済み（最優先タスクの変更、Phase 3の再設計）
- `docs/research_note_ja.md`: 要改訂（セクション3.5と5.5）
