# Dynamic F/G実験: 結果分析

## 実験概要

**目的:** 表現空間（F/G特徴量）とタスク空間（Reachingタスク）を整合させるため、時系列点群から学習した動的F/Gを用いてAgent Cの学習を改善する。

**仮説:** 静的形状ではなく動的な到達可能性を表現するF/Gを訓練すれば、Agent Cの学習が改善される。

## 実験設計

### Phase 1: 動的F/Gの訓練
- **データ:** ランダムポリシーで1000エピソードの時系列点群を収集
- **モデル:** TemporalPointNet (点群t, t+1 → affordance) + FunctorG (affordance + action → 次状態予測)
- **損失:** MSE(予測end-effector位置, 実際のend-effector位置)
- **結果:** F/G Loss 0.123 → 0.047（収束）

### Phase 2: Agent C v3の訓練
- **ベースライン:** 状態ベクトル（16次元）のみ
- **F/G-Enhanced:** 状態ベクトル（16次元） + affordance（32次元） + η（1次元） = 49次元
- **エピソード数:** 1500
- **環境:** PyBullet Reachingタスク（位置制御、3-DOF プレーナーアーム）

## 結果

### 定量的比較

| 指標 | ベースライン | F/G-Enhanced | 変化 |
|------|-------------|--------------|------|
| **最終平均報酬** | -4.26 | -4.78 | -12% |
| **最終成功率** | 0% | 0% | 0% |
| **最終平均距離** | 1.238m | 2.046m | +65% |
| **最高性能（エピソード800）** | 報酬27.63, 成功率25%, 距離0.385m | 報酬-3.44, 成功率0%, 距離0.922m | 大幅悪化 |
| **訓練時間** | 5.5分 | 65分 | 12倍 |

### 学習曲線の特徴

**ベースライン:**
- エピソード0-800: 急速な改善（報酬-4.6 → 27.63、成功率0% → 25%）
- エピソード800-1500: **性能崩壊**（報酬27.63 → -4.26、成功率25% → 0%）
- 学習は不安定だが、一時的に成功

**F/G-Enhanced:**
- 全エピソードを通じて**成功率0%**
- 報酬は-3～-5の範囲で停滞
- 距離は増加傾向（0.9m → 2.0m）
- **ηの発散:** 3.3 → 14.7（F/Gの予測誤差が累積）

## 根本原因の分析

### 1. ηの発散問題

ηは「予測と実際のend-effector位置の誤差」として定義されているが、訓練が進むにつれて**14.7まで増加**（初期値3.3の4.5倍）。これは以下を示唆：

- **F/Gの予測精度が低い:** 訓練時のloss 0.047は絶対値では小さいが、Reachingタスクの精度要求（成功閾値0.1m）に対しては不十分
- **分布シフト:** F/Gはランダムポリシーで訓練されたが、Agent Cは学習済みポリシーを使用。分布が異なるため予測が外れる
- **誤差の累積:** ηが大きいほど、Agent Cは「F/Gの予測は信頼できない」と学習し、F/G特徴量を無視する可能性

### 2. 高次元化の弊害

状態次元が16→49（3倍）に増加したことで：

- **サンプル効率の低下:** 同じ1500エピソードでは高次元空間を十分に探索できない
- **過学習のリスク:** affordance（32次元）とη（1次元）が実際には有用な情報を含んでいない場合、ノイズとして作用
- **最適化の困難:** 勾配が高次元空間で拡散し、収束が遅くなる

### 3. タスクミスマッチ（根本的問題）

**F/Gの設計思想とReachingタスクの不整合:**

| F/Gの想定 | Reachingタスクの実態 | ミスマッチ |
|-----------|---------------------|-----------|
| 形状→行動affordance | 位置→運動計画 | 形状情報は不要 |
| 物体の把持可能性 | 空間内の点への到達 | 把持は不要 |
| 複雑な形状理解 | 単純な幾何学（距離、角度） | 過剰な複雑性 |
| 点群（512点→128点） | 状態ベクトル（16次元）で十分 | 冗長な表現 |

**Reachingタスクに必要な情報:**
1. 現在のend-effector位置（3次元）
2. 目標位置（3次元）
3. 相対ベクトル（3次元）
4. 関節角度（3次元）
5. 関節角速度（3次元）

→ **合計16次元で完全に表現可能**

**F/Gが提供する情報:**
- affordance（32次元）: 「どの部分が把持可能か」→ Reachingには無関係
- η（1次元）: 「予測誤差」→ 学習が進むと発散し、信頼性が低下

### 4. ベースラインの性能崩壊

ベースラインも最終的に失敗しているが、これは**環境設計の問題**を示唆：

- **報酬関数の問題:** 距離改善量ベースの報酬は、局所最適解に陥りやすい
- **探索不足:** ε-greedyなどの探索戦略がない
- **ハイパーパラメータ:** 学習率、clip_epsilon、entropy_coefなどが最適化されていない

## 理論的含意

### 随伴の成立条件（改訂版）

**定理:** F/GとAgent Cの随伴が成立し、学習を改善するためには、以下の条件が**すべて**満たされる必要がある：

1. **表現空間の整合性:** F/Gの表現空間（affordance）がタスク空間（Agent Cの目標）と整合している
2. **情報の有用性:** F/Gが提供する情報が、タスク解決に**必要かつ十分**である
3. **予測精度:** ηの誤差がタスクの精度要求を満たす
4. **分布の一致:** F/Gの訓練分布とAgent Cの使用分布が一致している
5. **次元の適切性:** 追加される次元数が、サンプル効率の低下を上回る情報利得をもたらす

**本実験の評価:**

| 条件 | Step 2 v2（静的F/G） | Dynamic F/G | 評価 |
|------|---------------------|-------------|------|
| 1. 表現空間の整合性 | ❌ 静的形状 vs 動的運動 | ⚠️ 動的だが把持 vs 到達 | 部分的改善、不十分 |
| 2. 情報の有用性 | ❌ 形状情報は不要 | ❌ 把持情報は不要 | 改善なし |
| 3. 予測精度 | ❌ 不明（測定なし） | ❌ ηが発散（14.7） | 悪化 |
| 4. 分布の一致 | ⚠️ 不明 | ❌ ランダム vs 学習済み | 悪化 |
| 5. 次元の適切性 | ❌ 16→49（過剰） | ❌ 16→49（過剰） | 改善なし |

**結論:** Dynamic F/Gは条件1を部分的に改善したが、他の条件（特に2と3）が満たされていないため、学習を改善しなかった。

## 次のステップ

### 短期的改善（現在のF/G枠組み内）

1. **F/Gの再訓練:**
   - 学習済みポリシーで訓練データを生成（分布一致）
   - より大きなデータセット（5000エピソード以上）
   - より小さいaffordance次元（32→8）

2. **Agent Cの改善:**
   - エピソード数を増やす（1500→5000）
   - 探索戦略を追加（ε-greedy、entropy bonus）
   - ハイパーパラメータ最適化

3. **環境の改善:**
   - 報酬関数の見直し（sparse reward + potential-based shaping）
   - 成功閾値の緩和（0.1m → 0.15m）
   - カリキュラム学習（近い目標から遠い目標へ）

### 中期的方向転換（F/Gの再設計）

**アプローチA: タスク特化型F/G**
- affordanceを「到達可能性マップ」として再定義
- 入力: 現在のend-effector位置 + 目標位置
- 出力: 到達可能性スコア（0-1）+ 推奨方向（3次元ベクトル）
- 次元: 16 + 4 = 20（現実的）

**アプローチB: 階層的制御**
- F/G: 高レベルプランナー（サブゴール生成）
- Agent C: 低レベルコントローラー（サブゴールへの到達）
- 利点: 長期計画と短期制御の分離

**アプローチC: F/Gの放棄**
- Reachingタスクには状態ベクトル（16次元）で十分
- F/Gは**より複雑なタスク**（把持、組み立て、道具使用）で検証すべき
- 現在の失敗は「F/Gが無用」ではなく「タスクが単純すぎる」ことを示唆

### 長期的理論研究

1. **随伴の形式化:** 圏論的枠組みで随伴の成立条件を厳密に定義
2. **情報理論的分析:** F/Gが提供する情報量（mutual information）とタスク性能の関係
3. **メタ学習:** F/Gのアーキテクチャとタスクの相性を自動的に学習

## 結論

Dynamic F/Gの実験は、**表現空間とタスク空間の整合性**の重要性を実証したが、それだけでは不十分であることも示した。F/Gの成功には、情報の有用性、予測精度、分布の一致、次元の適切性など、複数の条件が**同時に**満たされる必要がある。

Reachingタスクは、F/Gの能力を検証するには**単純すぎる**可能性が高い。次のステップとして、より複雑なタスク（把持、組み立て、道具使用）でF/Gの有効性を検証することを推奨する。
