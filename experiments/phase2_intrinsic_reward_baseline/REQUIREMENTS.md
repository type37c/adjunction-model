# Phase 2実験：比較ηを用いたアフォーダンスの分解的理解 - 要件定義書

## 1. 概要

### 1.1. 背景
Phase 1では、Functor F/Gが基本的な形状（立方体、円柱、球）とそのアフォーダンス語彙（graspable, stackable, rollable, bouncy）を学習しました。Phase 2では、この学習済みF/Gを「凍結された測定器」として扱い、Agent Cが**比較η（Comparative η）**のメカニズムを手がかりとして、未知の複合的形状の構造を分解的に理解し、その部分ごとに適切なアフォーダンスを推測できるかを検証します。

### 1.2. 目的
本実験の主目的は、Agent Cが、オブジェクト全体に対する再構成誤差（η_whole）と、自身の注意選択によってフォーカスした部分に対する再構成誤差（η_part）を比較すること（比較η）で得られる内発的報酬（η軌跡の改善）に基づき、未知の形状を既知の構成要素へと分解する最適な注意選択方策を学習できることを実証することです。

## 2. スコープ

### 2.1. スコープ内
- 複合形状データセットの生成（容器、道具、構造物、複合オブジェクト）
- セマンティックセグメンテーションラベルの定義
- Agent Cの注意選択機構の実装
- 比較ηメカニズムの実装
- 内発的報酬の設計と実装
- DQNベースの学習アルゴリズム
- アフォーダンス推測の評価指標
- 訓練と評価のプロトコル

### 2.2. スコープ外
- シミュレーション環境での行動実行
- 外発的報酬（タスク達成報酬）
- Phase 1のF/Gの再訓練
- 新しいアフォーダンス語彙の追加
- リアルタイムロボット制御

## 3. 機能要件

### FR-01: 複合形状データセット
**要件**: 未知の複合形状を含むデータセットを生成する機能

**詳細**:
- 4つのカテゴリ（容器、道具、構造物、複合オブジェクト）の複合形状を生成
- 各オブジェクトにセマンティックセグメンテーションラベルを付与
- 各セグメントに期待されるアフォーダンスのground truthラベルを付与
- 訓練用と評価用に分離（訓練200サンプル、評価50サンプル）

**実装**: `src/data/composite_dataset.py`

### FR-02: 注意選択機構
**要件**: Agent Cがオブジェクトのどの部分に注目するかを選択する機能

**詳細**:
- 行動空間: {a_1, a_2, ..., a_n}（各a_iは特定のセグメントへの注意選択）
- 状態空間: オブジェクト全体の点群、現在の注意対象部分の点群、η軌跡の履歴
- Q-network: 各セグメントに対するQ値を出力
- ε-greedy探索戦略

**実装**: `src/models/agent_c_attention.py` の `AttentionAgent`

### FR-03: 比較ηメカニズム
**要件**: η_wholeとη_partを計算し、比較する機能

**詳細**:
- η_whole: オブジェクト全体の点群をF/Gに入力した際の再構成誤差
- η_part: Agent Cが選択した部分点群をF/Gに入力した際の再構成誤差
- 部分点群の正規化（中心化）処理
- η軌跡の履歴管理（過去10ステップ）

**実装**: `experiments/phase2_intrinsic_reward_baseline/run_phase2.py` の `_compute_eta`

### FR-04: 内発的報酬
**要件**: 比較ηに基づく内発的報酬を計算する機能

**詳細**:
- 報酬関数: r(t) = α · (η_whole(t) - η_part(t))
- αは正のスケーリング係数（デフォルト: 1.0）
- η_whole > η_part のとき正の報酬（良い注意選択）
- η_whole < η_part のとき負の報酬（悪い注意選択）

**実装**: `src/models/agent_c_attention.py` の `DQNTrainer.compute_intrinsic_reward`

### FR-05: DQN学習アルゴリズム
**要件**: 内発的報酬を最大化する注意選択方策を学習する機能

**詳細**:
- Deep Q-Network (DQN) アルゴリズム
- Target networkによる安定化
- Experience replay（オプション）
- ハイパーパラメータ:
  - 学習率: 1e-4
  - 割引率γ: 0.99
  - Target network更新頻度: 100ステップ
  - ε減衰: 1.0 → 0.1（0.995の減衰率）

**実装**: `src/models/agent_c_attention.py` の `DQNTrainer`

### FR-06: アフォーダンス推測評価
**要件**: Agent Cのアフォーダンス推測精度を評価する機能

**詳細**:
- 評価指標: Precision, Recall, F1スコア（将来実装）
- 現在: 訪問したセグメント数に基づく簡易評価
- Ground truthラベルとの比較
- エピソードごとの成功/失敗判定

**実装**: `experiments/phase2_intrinsic_reward_baseline/run_phase2.py` の `_compute_affordance_accuracy`

## 4. 非機能要件

### NFR-01: 性能
- エピソードあたりの実行時間: 1秒以内
- 1000エピソードの訓練を30分以内に完了
- GPU使用時のメモリ使用量: 4GB以下

### NFR-02: 安定性
- 訓練の発散を防止（gradient clipping、target network）
- ε-greedy探索による適切な探索と活用のバランス
- η値の異常値に対する堅牢性

### NFR-03: 再現性
- 乱数シードの固定（訓練: 42、評価: 123）
- チェックポイントの定期保存（100エピソードごと）
- 訓練履歴の記録（報酬、損失、精度、ε値）

### NFR-04: 保守性
- モジュール化されたコード構造
- 明確なインターフェース設計
- ドキュメント化されたハイパーパラメータ

### NFR-05: 拡張性
- 新しいカテゴリの複合形状を容易に追加可能
- 異なる学習アルゴリズム（SAC、PPOなど）への切り替えが容易
- アフォーダンス評価指標の拡張が容易

## 5. 技術的課題と対策

### 課題1: 部分点群の正規化
**課題**: F/Gはオブジェクト中心の座標系で訓練されているため、Agent Cが選択した部分点群をF/Gに入力する前に、位置と向きを正規化する必要がある。

**対策**: 部分点群の重心を原点に移動させる正規化処理を実装（`_extract_segment_points`）。

### 課題2: 報酬スケーリング
**課題**: 内発的報酬のスケール（係数α）が学習の安定性に大きく影響する可能性がある。

**対策**: αをハイパーパラメータとして調整可能にし、実験的に最適値を探索（デフォルト: 1.0）。

### 課題3: 探索と活用のバランス
**課題**: エージェントが未知の構造を効果的に発見するための探索戦略と、既知の有望な部分を活用する戦略のバランスを適切に調整する必要がある。

**対策**: ε-greedy探索戦略を採用し、εを1.0から0.1まで徐々に減衰させる（減衰率: 0.995）。

## 6. 評価基準

### 成功基準
以下の条件を全て満たす場合、Phase 2実験は成功とみなされます：

1. **注意選択の収束**: Agent Cの注意選択がランダムから有意に改善し、特定のパターンが観察される
2. **比較ηの活用**: η_whole > η_part となる注意選択が増加し、内発的報酬が向上する
3. **アフォーダンス推測の精度**: 評価データセットにおいて、ランダムベースラインを有意に上回る精度を達成
4. **分解的理解の証拠**: ログ分析により、Agent Cが複合オブジェクトを構成要素に分解する過程が観察される

### 失敗条件
以下のいずれかの結果となった場合、実験は失敗とみなされ、仮説または設計の見直しが必要となります：

1. Agent Cの注意選択がランダムなまま収束せず、特定の構造を発見できない
2. Agent Cが単一の部分に固執し、オブジェクト全体を探索する行動が生まれない
3. 最終的なアフォーダンス予測の精度が、ランダムベースラインを有意に上回らない
4. 訓練が不安定で、損失が発散する

## 7. 成果物

### 7.1. コード
- `src/data/composite_dataset.py`: 複合形状データセット
- `src/models/agent_c_attention.py`: 注意選択Agent C
- `experiments/phase2_intrinsic_reward_baseline/run_phase2.py`: 実験実行スクリプト

### 7.2. チェックポイント
- `checkpoints/phase2_episode_*.pt`: 訓練チェックポイント（100エピソードごと）

### 7.3. 結果
- `results.json`: 訓練履歴（報酬、損失、精度、ε値）
- `RESULTS.md`: 実験結果レポート

### 7.4. ドキュメント
- `REQUIREMENTS.md`: 本要件定義書
- `README.md`: 実験の概要と実行方法

## 8. 実行手順

### 8.1. 環境準備
```bash
# 依存パッケージのインストール
sudo pip3 install torch trimesh

# Phase 1チェックポイントの確認
ls ../../experiments/phase1_basic_adjunction/checkpoints/phase1_final.pt
```

### 8.2. 実験実行
```bash
cd experiments/phase2_intrinsic_reward_baseline
python -u run_phase2.py > phase2.log 2>&1 &
```

### 8.3. 進捗確認
```bash
tail -f phase2.log
```

## 9. 期待される結果

成功した場合、Agent Cは未知の形状（例：取っ手付き立方体）に対して、以下の行動パターンを示すことが期待されます：

1. **初期探索**: まず全体を見て高いη_wholeを観測
2. **注意の切り替え**: 注意を「立方体部分」と「取っ手部分」に順番に切り替える
3. **低いη_partの発見**: 各部分で低いη_partを観測（既知の構成要素として認識）
4. **アフォーダンス推測**: 「立方体部分」からstackable、「取っ手部分」からgraspableを安定して予測

これは、エージェントが比較ηのメカニズムを通じて、複合オブジェクトをその構成要素に正しく分解し、それぞれの文脈に応じたアフォーダンスを推測できたことを示します。
