# Step 2 v2: Agent C再設計 - 実験結果分析

## 実験概要

**目的:** F/G特徴量（affordance + η）がAgent Cの学習を促進するかを検証

**環境:** PyBullet 3-DOF planar robot arm, reaching task
- 位置制御（トルク制御から改善）
- 改善された報酬関数（距離改善量ベース）
- 豊かな状態表現（end-effector位置、相対ベクトル）

**Agent C v2:**
- MLP Actor-Critic（LSTMなし、マルコフ的タスク）
- PPO v2（報酬正規化、GAE）
- F/G-enhanced版: affordance(32次元) + η(1次元) を状態に追加
- 二重報酬: r(t) = r_ext(t) + α_int·(-Δη(t))

## 実験結果

### Baseline (状態ベクトルのみ)
- エピソード数: 1500
- 訓練時間: 5.1分
- **最終平均報酬: 2.69**
- **成功率: 3%**
- **最終平均距離: 0.4534**
- 総成功: 61/1500
- ベスト平均報酬: 18.87

### F/G-Enhanced (状態 + affordance + η)
- エピソード数: 1500
- 訓練時間: 67.0分
- **最終平均報酬: -6.22**
- **成功率: 0%**
- **最終平均距離: 1.2705**
- 総成功: 21/1500
- ベスト平均報酬: 18.83
- 平均η: 3.398

## 主要な観察

### 1. F/G-Enhancedがベースラインより悪化

**グラフから読み取れる事実:**
- **初期学習（0-200エピソード）:** F/G版は報酬が急速に上昇し、成功率も16%に達した（ベースラインは0%）
- **中期（200-600エピソード）:** F/G版の性能が崩壊し、報酬が負に転じた
- **後期（600-1500エピソード）:** F/G版は回復せず、距離が2.0-2.5で停滞

**ベースラインの学習曲線:**
- 安定した改善（距離1.7→0.45）
- 成功率は低いが（3%）、一貫して前進
- 報酬の分散が小さい

### 2. ηの挙動

- ηは3.38-3.40の範囲で安定（ほぼ変化なし）
- Δη（ηの変化率）が内発的報酬として機能していない可能性
- ηが学習に有用な信号を提供していない

### 3. 訓練時間の差

- F/G版はベースラインの13倍遅い（67分 vs 5分）
- 点群処理（128点×FunctorF）がボトルネック
- 実用性の観点で問題

## 失敗の仮説

### 仮説1: 状態空間の次元呪い
- ベースライン: 16次元
- F/G-enhanced: 49次元（16 + 32 + 1）
- 高次元空間での探索が困難になった可能性
- PPOのハイパーパラメータ（特にmini_batch_size=64）が49次元に対して不十分

### 仮説2: F/G特徴量のミスマッチ
- F/Gは**静的な点群**（物体の形状）で訓練された
- Reachingタスクは**動的な相対位置**が重要
- F/Gのaffordance表現が動的タスクに適合していない
- ηは「握り（grip）」の指標だが、reachingには無関係

### 仮説3: 内発的報酬の設計ミス
- α_int = 0.1 が大きすぎた可能性
- Δηの符号（負=改善）が学習を混乱させた
- 外発的報酬（距離改善）と内発的報酬（Δη）が競合

### 仮説4: 初期学習の成功が過学習を誘発
- 初期の急速な改善（0-200エピソード）が局所最適に陥った
- F/G特徴量が初期探索を加速したが、長期的には有害だった
- ベースラインの「遅いが安定した学習」の方が最終的に優れていた

### 仮説5: 点群の品質問題
- 128点の点群がタスクに対して粗すぎる
- カメラの視点が固定で、動的な情報を捉えられない
- 点群がend-effectorの動きを反映していない

## 理論的含意

### 随伴の成立条件

**価値関数分析v3の予測:**
> 「F/Gという川床の上をAgent Cという水が流れると、はじめて随伴が成立する」

**実験結果:**
- 随伴は成立しなかった（むしろ阻害された）
- F/Gの「川床」がReachingタスクの「水の流れ」に適合していなかった

**修正された理解:**
> 「随伴が成立するには、F/Gの表現空間とAgent Cのタスク空間が整合している必要がある」

### ηの役割の再考

**当初の仮説:**
- η（再構成誤差）は「握り（grip）」の指標
- Δηは学習の進捗を示す内発的報酬

**実験結果:**
- ηはほぼ一定（3.38-3.40）で変化しない
- Δηは学習信号として機能していない

**修正された理解:**
- ηは**静的な形状理解**の指標であり、**動的な行動**には無関係
- Reachingタスクでは物体の形状ではなく、**相対位置と速度**が重要

## 次のステップ

### 短期的改善案

1. **F/G特徴量の選択的使用**
   - affordanceのみを使用（ηを除外）
   - 内発的報酬を削除（α_int = 0）

2. **ハイパーパラメータ調整**
   - mini_batch_size を128に増加
   - learning rate を1e-4に減少
   - clip_epsilon を0.1に減少（保守的な更新）

3. **状態表現の改善**
   - affordanceを低次元（8-16次元）に圧縮
   - PCAやAutoencoderで次元削減

### 長期的方向性

1. **タスク適合型F/Gの再訓練**
   - 動的な点群（時系列）でF/Gを訓練
   - Reachingタスクに特化したaffordance表現

2. **階層的強化学習**
   - F/Gを高レベル目標設定に使用
   - Agent Cを低レベル制御に特化

3. **代替アプローチ**
   - End-to-end学習（点群→行動）
   - Transformer-based architecture

## 結論

F/G特徴量の統合は、当初の予想に反してAgent Cの学習を**阻害**した。これは、F/Gの表現空間（静的形状）とReachingタスクの要求（動的相対位置）のミスマッチに起因すると考えられる。

**重要な教訓:**
> 「随伴の成立には、表現空間とタスク空間の整合性が必須である」

ベースラインの成功（報酬2.69、成功率3%）は、環境再設計（位置制御、改善された報酬関数）の有効性を示している。F/G統合の失敗は、理論的枠組みの限界ではなく、**実装の詳細**（特徴量の選択、ハイパーパラメータ）に起因する可能性が高い。
