# 内発的報酬ベースライン実験：結果分析

**日付**: 2026年2月17日  
**実験**: Episode-Based Training (100エピソード × 10ステップ)

## 1. 実験の成功：η爆発の完全な防止

エピソードベースの訓練構造により、**η（Coherence）の爆発を完全に防止することに成功した**。

### 1.1 η（Coherence）の安定性

| 実験 | 訓練構造 | η の推移 | 結果 |
|:---|:---|:---|:---|
| **Epoch-based（失敗）** | 50エポック × 1000ステップ | 0.47 → 96兆 | 爆発 |
| **Episode-based（今回）** | 100エピソード × 10ステップ | 0.437 → 0.437 (Δ -0.0004) | **安定** |
| **2/13実験（成功）** | 100エピソード × 10ステップ | 0.447 → 0.437 (Δ -0.0003) | 安定 |

今回の実験では、ηは0.42〜0.46の範囲で安定的に推移し、2/13実験と同等の安定性を達成した。これは、**エピソードリセットが「睡眠」として機能し、Agent Cが破壊的な戦略を学習する前に状態をリセットする**という設計原則が正しかったことを証明している。

### 1.2 ε（Epsilon）の安定性

ε（Affordance Slack）も0.0007〜0.0013の範囲で安定しており、爆発は見られなかった。

## 2. 課題：内発的報酬とValenceの成長不足

安定性は達成したが、**学習のダイナミクスが2/13実験と大きく異なる**。

### 2.1 内発的報酬の成長率

| 実験 | R_intrinsic 初期 | R_intrinsic 最終 | 成長率 |
|:---|:---|:---|:---|
| **2/13実験** | 0.017 | 0.340 | **+1900%** |
| **今回の実験** | 0.047 | 0.055 | **+17%** |

今回の実験では、R_intrinsicの成長が2/13実験の約1/100に留まった。グラフを見ると、最初の40エピソードでは横ばいで、エピソード80以降にわずかに上昇している。

### 2.2 Valenceの停滞

| 実験 | Valence 初期 | Valence 最終 | 変化 |
|:---|:---|:---|:---|
| **2/13実験** | 0.580 | 0.661 | **+14%** |
| **今回の実験** | -0.0001 | -0.0001 | **ほぼゼロ** |

Valenceは、2/13実験では0.58から0.66へと明確に成長したが、今回の実験ではほぼゼロで停滞した。グラフを見ると、エピソード40付近で一度0.0008まで上昇したが、その後再びゼロ付近に戻っている。

## 3. 原因の仮説

### 3.1 仮説1：Valence Memoryの初期化

2/13実験と今回の実験で、Valence Memoryの初期値が異なる可能性がある。2/13では初期Valenceが0.58であったのに対し、今回はほぼゼロからスタートしている。

Valence Memoryは、Agent Cの「自信」や「有能感」を表す。初期値が低いと、Agent Cは自信を持てず、探索が消極的になり、結果として内発的報酬の成長が抑制される可能性がある。

### 3.2 仮説2：報酬スケーリングの違い

2/13実験では、報酬スケーリングが適用されていた可能性がある。今回の実験では`reward_scale=1.0`（スケーリングなし）を使用したが、2/13では異なる値が使用されていたかもしれない。

### 3.2 仮説3：F/Gの事前訓練状態

2/13実験では、F/GがPhase 1（基本的な随伴訓練）やPhase 2（Slack保存訓練）を経て、ある程度の性能を持った状態で固定されていた可能性がある。今回の実験では、F/Gはランダム初期化のまま固定されている。

F/Gの性能が低い場合、ηが常に高く（再構成誤差が大きく）、Competence報酬（ηの減少に対する報酬）が得られにくい。結果として、内発的報酬の成長が抑制される。

## 4. 次のステップ

### 4.1 優先度1：Valence Memoryの初期化を確認

2/13実験のコードを確認し、Valence Memoryの初期値がどのように設定されていたかを調査する。もし2/13で高い初期値が使用されていたなら、今回の実験でも同じ初期値を使用して再実験する。

### 4.2 優先度2：F/Gの事前訓練

F/Gをランダム初期化のまま固定するのではなく、Phase 1（基本的な随伴訓練）を実施してから固定する。これにより、F/Gがある程度の再構成性能を持った状態で、Agent Cの訓練を開始できる。

### 4.3 優先度3：報酬スケーリングの調査

2/13実験で使用されていた報酬スケーリングの値を確認し、必要に応じて適用する。

## 5. 結論

エピソードベースの訓練構造により、**η爆発を完全に防止し、随伴構造の安定性を確保することに成功した**。これは、「時間的持続に適切な境界を設ける」という設計原則の正しさを証明している。

しかし、**内発的報酬とValenceの成長が2/13実験と比べて大幅に低い**という課題が残っている。これは、Valence Memoryの初期化、F/Gの事前訓練状態、報酬スケーリングなどの違いに起因すると考えられる。

次のステップとして、これらの要因を調査し、2/13実験の完全な再現を目指す。
