"""
Intrinsic Reward Baseline Experiment

This script reproduces the 2/13 experiment with a clean codebase:
- F/G frozen (fixed weights)
- Agent C driven by intrinsic rewards only (Competence + Novelty)
- No Affordance Loss
- Curiosity disabled (α=0.0)

Expected results (from 2/13):
- Intrinsic reward: +1584%
- Value function: +51000%
- Valence growth: 0.58 → 0.667 (accelerating)
- Coherence stable: ~0.43
- Competence reward: Primary driving force (59.5%)
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import yaml
import os
import sys
import json
from pathlib import Path

sys.path.append('/home/ubuntu/adjunction-model')

from src.models.adjunction_model import AdjunctionModel
from src.data.synthetic_dataset import SyntheticAffordanceDataset
from src.training.train_intrinsic_reward import IntrinsicRewardTrainer


def load_config(config_path: str) -> dict:
    """Load experiment configuration from YAML file."""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def create_model(config: dict, device: torch.device) -> AdjunctionModel:
    """Create AdjunctionModel from config."""
    model_config = config['model']
    
    model = AdjunctionModel(
        num_affordances=model_config['num_affordances'],
        num_points=model_config['num_points'],
        f_hidden_dim=model_config['f_hidden_dim'],
        g_hidden_dim=model_config['g_hidden_dim'],
        agent_hidden_dim=model_config['agent_c']['rssm_hidden_dim'],
        agent_state_dim=model_config['agent_c']['rssm_state_dim'],
        valence_dim=model_config['agent_c']['valence_dim']
    )
    
    return model.to(device)


def create_dataloader(config: dict) -> DataLoader:
    """Create training dataloader."""
    dataset_config = config['dataset']
    training_config = config['training']
    
    dataset = SyntheticAffordanceDataset(
        num_shapes=dataset_config['num_shapes'],
        num_points=dataset_config['num_points'],
        noise_std=dataset_config['noise_std']
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=training_config['batch_size'],
        shuffle=True
    )
    
    return dataloader


def main():
    # Load configuration
    config_path = os.path.join(os.path.dirname(__file__), 'config.yaml')
    config = load_config(config_path)
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Create results directory
    results_dir = os.path.join(os.path.dirname(__file__), 'results')
    os.makedirs(results_dir, exist_ok=True)
    os.makedirs(os.path.join(results_dir, 'checkpoints'), exist_ok=True)
    os.makedirs(os.path.join(results_dir, 'logs'), exist_ok=True)
    
    # Create model
    print("\nCreating model...")
    model = create_model(config, device)
    
    # Create dataloader
    print("Creating dataloader...")
    dataloader = create_dataloader(config)
    
    # Create trainer
    print("Creating trainer...")
    training_config = config['training']
    trainer = IntrinsicRewardTrainer(
        model=model,
        device=device,
        lr=training_config['optimizer']['learning_rate'],
        lambda_kl=training_config['loss']['kl_weight'],
        lambda_value=1.0,
        freeze_fg=config['model']['fg_frozen']
    )
    
    # Training loop
    print(f"\nStarting training for {training_config['epochs']} epochs...")
    print("="*60)
    
    metrics_history = {
        'loss': [],
        'kl_loss': [],
        'value_loss': [],
        'intrinsic_reward': [],
        'competence_reward': [],
        'novelty_reward': [],
        'valence': [],
        'coherence': [],
        'eta': [],
        'epsilon': []
    }
    
    for epoch in range(training_config['epochs']):
        # Train
        metrics = trainer.train_epoch(dataloader, epoch)
        
        # Log metrics
        for key in metrics_history.keys():
            if key in metrics:
                metrics_history[key].append(metrics[key])
        
        # Print progress
        if (epoch + 1) % config['logging']['log_interval'] == 0:
            print(f"\nEpoch {epoch+1}/{training_config['epochs']}")
            print(f"  Loss: {metrics['loss']:.6f}")
            print(f"  KL Loss: {metrics['kl_loss']:.6f}")
            print(f"  Value Loss: {metrics['value_loss']:.6f}")
            print(f"  Intrinsic Reward: {metrics['intrinsic_reward']:.6f}")
            print(f"    - Competence: {metrics['competence_reward']:.6f}")
            print(f"    - Novelty: {metrics['novelty_reward']:.6f}")
            print(f"  Valence: {metrics['valence']:.6f}")
            print(f"  Coherence: {metrics['coherence']:.6f}")
            print(f"  η (unit): {metrics['eta']:.6f}")
            print(f"  ε (counit): {metrics['epsilon']:.6f}")
        
        # Save checkpoint
        if (epoch + 1) % config['logging']['save_interval'] == 0:
            checkpoint_path = os.path.join(
                results_dir, 'checkpoints', f'model_epoch_{epoch+1}.pt'
            )
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'metrics': metrics
            }, checkpoint_path)
    
    # Save final metrics
    metrics_path = os.path.join(results_dir, 'metrics.json')
    with open(metrics_path, 'w') as f:
        json.dump(metrics_history, f, indent=2)
    
    print("\n" + "="*60)
    print("Training completed!")
    print(f"Results saved to: {results_dir}")
    
    # Print final summary
    print("\nFinal Metrics:")
    print(f"  Intrinsic Reward: {metrics_history['intrinsic_reward'][0]:.6f} → {metrics_history['intrinsic_reward'][-1]:.6f}")
    print(f"    Change: {(metrics_history['intrinsic_reward'][-1] / metrics_history['intrinsic_reward'][0] - 1) * 100:.1f}%")
    print(f"  Valence: {metrics_history['valence'][0]:.6f} → {metrics_history['valence'][-1]:.6f}")
    print(f"    Change: {(metrics_history['valence'][-1] / metrics_history['valence'][0] - 1) * 100:.1f}%")
    print(f"  Coherence: {metrics_history['coherence'][0]:.6f} → {metrics_history['coherence'][-1]:.6f}")
    
    # Compare with 2/13 results
    print("\nComparison with 2/13 results:")
    print("  Expected intrinsic reward growth: +1584%")
    print("  Expected valence growth: +15% (0.58 → 0.667)")
    print("  Expected coherence: stable (~0.43)")


if __name__ == '__main__':
    main()
