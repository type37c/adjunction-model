# Intrinsic Reward Baseline Experiment

## Driving Mechanism
- [ ] Loss function-driven
- [x] Intrinsic reward-driven (Competence + Novelty)

## F/G State
- [x] Frozen (using fixed weights)
- [ ] Learnable

## Goal

To reproduce the 2/13 experiment in a clean codebase and verify that **intrinsic motivation alone** (without external objectives like Affordance Loss) can drive Agent C's learning and potentially lead to the emergence of suspension structures.

## Hypothesis

When driven purely by intrinsic rewards (Competence + Novelty), Agent C will:
1. Show significant growth in intrinsic reward (+1000% or more)
2. Exhibit accelerating Valence growth (indicating purpose formation)
3. Maintain stable Coherence (not fleeing from breakdowns)
4. Demonstrate that Competence reward ("attending to breakdowns") is the primary driving force

## Design

### Model Configuration
- **F/G**: Frozen (fixed weights from initialization)
- **Agent C**: Full RSSM with Valence Memory
- **Value Function**: Estimates expected future intrinsic rewards

### Training Paradigm: Value-Based Reinforcement Learning

**Critical difference from Phase 2 Slack**: This experiment uses **value-based RL** instead of loss function minimization.

**Training process (3 phases per episode)**:
1. **Trajectory collection** (no gradient): Agent C interacts with shapes for multiple steps, collecting states and intrinsic rewards
2. **Value function update** (TD learning): Update V(state) using temporal difference learning
3. **Agent C update** (value maximization): Train Agent C to maximize V(state) via gradient ascent

**Why this matters**: Agent C learns to "seek states with high expected future rewards" rather than "minimize current error". This is the foundation for purpose emergence.

### Training Configuration
- **Driving force**: Expected future intrinsic rewards (not external loss)
- **Episode structure**: 5 steps per episode (temporal continuity)
- **Intrinsic rewards**:
  - **Competence**: weight: 0.6, type: "attention_to_breakdown"
    - Definition: `R_competence = coherence_curr Ã— attention Ã— 100`
    - Rationale: Reward "attending to breakdowns" (engaging with difficulty)
  - **Novelty**: weight: 0.4
    - Definition: `R_novelty = KL_divergence Ã— 0.1`
    - Rationale: Reward unexpected discoveries
  - **Curiosity**: **Disabled** (Î±=0.0)
    - Rationale: 2/13 experiment found Curiosity had sign issues

### Hyperparameters
- Epochs: 50
- Batch size: 4
- Episode length: 5 steps
- Agent C learning rate: 0.0001
- Value function learning rate: 0.001
- Discount factor (Î³): 0.99
- Num shapes: 100
- Num points: 256

## Implementation

### Files
- `config.yaml`: Experiment configuration
- `run.py`: Training script
- `analyze.py`: Analysis and visualization script
- `results/`: Output directory (created during training)
  - `checkpoints/`: Model checkpoints
  - `logs/`: Training logs
  - `metrics.json`: Recorded metrics
  - `analysis.png`: Visualization

### Dependencies
- `src/training/train_agent_value_based.py`: ValueBasedAgentTrainer (from legacy_code)
- `src/models/value_function.py`: ValueFunction and TDLearner (from legacy_code)
- `src/models/adjunction_model.py`: AdjunctionModel (with freeze_fg() method)
- `src/models/agent_c.py`: AgentC
- `src/models/intrinsic_reward.py`: IntrinsicRewardComputation
- `src/models/valence_v2.py`: ValenceMemoryV2
- `src/data/shape_dataset.py`: ShapeDataset

## Metrics Tracked

- [x] Intrinsic rewards (total, Competence contribution, Novelty contribution)
- [x] Valence (mean, growth rate)
- [x] Coherence (mean, stability)
- [x] Î· (perceptual slack / unit signal) time evolution
- [x] Îµ (semantic slack / counit signal) time evolution
- [x] KL divergence
- [x] Value function loss

## How to Run

### 1. Run the experiment

```bash
cd /home/ubuntu/adjunction-model
python experiments/intrinsic_reward_baseline/run.py
```

This will train for 100 epochs and save results to `experiments/intrinsic_reward_baseline/results/`.

### 2. Analyze the results

```bash
python experiments/intrinsic_reward_baseline/analyze.py
```

This will generate visualizations and print a comparison with 2/13 results.

## Results

**Status**: ðŸš§ Not yet run

### Expected Results (from 2/13)

| Metric | Initial | Final | Change |
|:---|---:|---:|---:|
| Intrinsic Reward | 0.025 | 0.421 | **+1584%** |
| Value Function | 0.013 | 6.639 | **+51000%** |
| Valence | 0.58 | 0.667 | **+15%** (accelerating) |
| Coherence | 0.43 | 0.43 | 0% (stable) |

**Reward Composition (2/13, Final Epoch)**:
- Competence: 59.5% (primary driving force)
- Novelty: 40.5%

### Key Findings

(To be filled in after experiment completion)

### Visualizations

(To be generated by `analyze.py`)

### Comparison with Other Experiments

This experiment tests the **core hypothesis** of the project: Can structure emerge from intrinsic motivation alone, without external objectives?

**Comparison with `slack_affordance_loss/`**:
- `slack_affordance_loss/`: Affordance Loss drives training â†’ Slack preserved
- `intrinsic_reward_baseline/`: Intrinsic rewards drive training â†’ Structure emerges?

The key difference is that `slack_affordance_loss/` uses an external objective (Affordance Loss), which is an extension of existing AI paradigms. This experiment tests whether intrinsic motivation alone is sufficient.

## Conclusion

(To be filled in after experiment completion)

---

**Status**: ðŸš§ Implemented, not yet run  
**Date Created**: 2026-02-16  
**Date Completed**: TBD  
**Experimenter**: Manus AI Agent
