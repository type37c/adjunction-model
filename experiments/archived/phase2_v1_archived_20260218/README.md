# Phase 2: 内発的報酬ベースライン実験

## 目的

Phase 1で事前訓練されたF/Gを固定し、Agent Cが**内発的報酬のみ**で駆動される環境で学習できることを検証する。これは、Phase Aの重要なマイルストーンであり、後続のPhase Bにおける注意選択実装の基盤となる。

## 実験設計

### 前提条件
- **F/Gは固定**: Phase 1で訓練済みのF/Gチェックポイントを読み込み、パラメータを凍結する。
- **外部目的なし**: アフォーダンス損失などの外部目的関数は使用しない。
- **価値関数ベース学習**: Agent Cは、内発的報酬の期待累積値を最大化するように学習する。

### 内発的報酬の定義

内発的報酬は、**Competence Reward**と**Novelty Reward**の2つの成分から構成される。

```python
R_intrinsic = R_competence + R_novelty

# Competence Reward: ηの改善
R_competence = η(t-1) - η(t)  # ηが減少すれば正の報酬

# Novelty Reward: 新しい状態への探索
R_novelty = -log(visitation_count(state))  # 訪問回数が少ない状態ほど高い報酬
```

### 訓練サイクル

Phase 2の訓練は、以下の3段階のサイクルで進行する。

1. **軌跡収集 (Trajectory Collection)**: Agent Cが現在の方策に従って環境と相互作用し、経験を収集する。
2. **価値関数更新 (Value Function Update)**: TD学習により、価値関数を更新する。
3. **Agent C更新 (Agent C Update)**: 価値関数を固定し、Agent Cの方策を価値最大化の方向に更新する。

## 評価指標

- **内発的報酬の成長**: エピソードごとの累積内発的報酬が増加しているか。
- **Competence寄与率**: 内発的報酬のうち、Competence Rewardが占める割合。
- **η（再構成誤差）**: F/Gが固定されているため、ηの平均値は大きく変化しないはずだが、Agent Cがηの低い（整合性の高い）状態を選好するようになるかを観察する。
- **価値関数の学習**: TD誤差が減少し、価値関数が安定して学習されているか。

## 期待される結果

Phase 1で安定したF/Gが得られていれば、Agent Cは内発的報酬に駆動されて以下のような振る舞いを学習すると期待される。

- ηが低い（整合性が高い）状態を探索・維持する。
- 新規性と整合性のバランスを取りながら、長期的な報酬を最大化する方策を獲得する。
- 価値関数が安定して学習され、TD誤差が減少する。

この実験の成功は、Phase Bにおける注意選択メカニズムの実装に向けた重要な基盤となる。
