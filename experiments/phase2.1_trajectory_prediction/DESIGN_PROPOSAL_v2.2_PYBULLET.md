# Phase 2.1 実験設計提案 v2.2：PyBulletによる物理的グラウンディング

**バージョン**: v2.2  
**日付**: 2026-02-19  
**ステータス**: 設計中

## 1. 背景と動機

### 1.1. これまでの経緯

- **Phase 1**: F/G（形状再構成モデル）の事前訓練を完了。基本3形状で79.3%のη削減を達成。
- **Phase 2 v1**: Agent Cによる注意選択と比較ηメカニズムを実装したが、批判的分析により設計上の問題が明らかになった（タスクが単純すぎ、報酬設計が単調、状態遷移の概念が欠如）。
- **Phase 2.1 v2.1設計**: 軌跡予測ベースの価値関数とLSTMによる注意履歴の保持を提案したが、物理的グラウンディングが欠如していた。

### 1.2. 理論的課題の明確化

`phase3_reward_discussion.pdf`での議論により、以下の核心的な問題が明らかになった：

1. **外発的報酬 vs 内発的報酬のジレンマ**: 
   - ηの最小化（内発的報酬）だけでは「理解」に留まり、「使う」という目的指向の行動は生まれない
   - タスク報酬（外発的報酬）を与えると「目的の創発」という理論的目標と矛盾する

2. **随伴の成立には目的が必要**:
   - 目的が存在しなければ、F/Gは単なる形状再構成装置に留まり、Agent Cとの相互作用（随伴）が成立しない
   - 「袋に物を入れる」という行動が生まれるには、「物を入れたい」という目的が必要

3. **タスク報酬は目的の正当な表現手段**:
   - エージェントの目的を実装レベルで表現する上で、タスク報酬は最も直接的で正当な方法
   - 理論からの逸脱ではなく、「目的を通した随伴」を実装に落とし込むための現実的なアプローチ

### 1.3. Phase 2.1 v2.2の方針

本設計では、上記の理論的議論を踏まえ、以下の方針で実験を設計する：

1. **PyBullet物理シミュレーション環境**を導入し、アフォーダンスを物理的相互作用としてグラウンディングする
2. **報酬の二重構造**を採用：内発的報酬（η改善）とタスク報酬（目的達成）を両立させる
3. **F/GとAgent Cの明確な統合パイプライン**を実装し、随伴を成立させる
4. **複数のインタラクションが可能なタスク**を設計し、アフォーダンス推測能力を真に検証する

## 2. 実験設計

### 2.1. 環境構築：PyBullet物理シミュレーション

#### 2.1.1. シーン構成

- **テーブル**: 平面（plane）として実装
- **オブジェクト**: 複数の操作可能なオブジェクトを配置
  - 初期段階：箱、ボウル、カップなど（Phase 1で訓練済みの形状から選択）
  - 発展段階：引き出し、ドア、レバーなど（複数のインタラクション方法が可能なもの）
- **ロボットアーム**: KUKA iiwaアーム + グリッパー
- **カメラ**: 複数視点からの観測が可能（エージェントが注意選択で視点を変更できる）

#### 2.1.2. 観測空間

各タイムステップで以下を取得：

1. **点群データ**: PyBulletのカメラから深度画像を取得し、点群に変換（1024点）
2. **ロボット状態**: 関節角度、エンドエフェクタ位置、グリッパー開閉状態
3. **オブジェクト状態**: 位置、姿勢、速度（タスク成功判定用、エージェントには直接与えない）

#### 2.1.3. 行動空間

Agent Cの行動空間は以下の2つの要素から構成される：

1. **注意選択** (離散): どの視点/部分に注目するか（例：8方向の視点選択）
2. **物理的行動** (連続): ロボットアームの制御
   - エンドエフェクタの目標位置 (x, y, z)
   - グリッパーの開閉 (open/close)
   - 実装：DDPG/SACなどの連続制御アルゴリズム

### 2.2. F/GとAgent Cの統合パイプライン

Phase 2.1の核心は、F/GとAgent Cの相互作用（随伴）を明確に実装することである。

#### 2.2.1. 各タイムステップの処理フロー

```
1. 環境から点群 x_t を観測
2. F/Gで再構成誤差を計算：
   - η_whole = ||G(F(x_t)) - x_t||²  （全体形状の再構成誤差）
   - η_part = ||G(F(x_t^attended)) - x_t^attended||²  （注目部分の再構成誤差）
3. 内発的報酬を計算：
   - r_intrinsic(t) = α · (η_whole - η_part)  （比較η）
4. Agent Cが次の行動を決定：
   - 入力：F(x_t)（アフォーダンス特徴）、ロボット状態、LSTM隠れ状態
   - 出力：注意選択 a_attention、物理的行動 a_physical
5. 物理的行動を実行し、環境が変化
6. タスク報酬を計算：
   - r_task(t) = タスク達成度に基づく報酬（後述）
7. 総報酬を計算：
   - r(t) = α · r_intrinsic(t) + β · r_task(t)
8. Agent Cを更新（LSTM状態も更新）
```

#### 2.2.2. F/Gの役割

- **Phase 1で事前訓練済み**のF/Gを使用（`phase1_final.pt`をロード）
- F/Gのパラメータは**固定**（Phase 2.1では更新しない）
- F/Gは「川床」として機能：ゆっくりと進化した世界モデルを提供
- F(x)のアフォーダンス特徴がAgent Cの状態表現の一部となる

#### 2.2.3. Agent Cの役割

- **LSTM-based Actor-Critic**アーキテクチャ
- 「水の流れ」として機能：価値関数に基づいて速い時間スケールで行動
- 注意選択と物理的行動の両方を制御
- η軌跡の予測と改善を通じて、アフォーダンスの理解を深める

### 2.3. タスク設計：複数インタラクションが可能なタスク

Phase 2.1では、単純な「Picking」ではなく、**複数の異なる関わり方が可能**なタスクを設計する。

#### 2.3.1. タスク例1：「Container Filling」（容器充填）

- **目標**: ボウルまたはカップに小さなオブジェクト（ボール）を入れる
- **必要なアフォーダンス理解**:
  - 容器の開口部の位置と向き
  - 容器の深さ（どこまで入れられるか）
  - 容器の安定性（倒れないか）
- **複数のインタラクション**:
  - 容器を持ち上げる
  - 容器を傾ける
  - ボールを掴む
  - ボールを容器に入れる

#### 2.3.2. タスク例2：「Articulated Object Manipulation」（関節物体操作）

- **目標**: 引き出しを開ける、またはドアを開ける
- **必要なアフォーダンス理解**:
  - 取っ手の位置
  - 引く/押す方向
  - 関節の軸
- **複数のインタラクション**:
  - 取っ手を掴む
  - 引く
  - 押す
  - 回す

#### 2.3.3. タスク報酬の設計

タスク報酬は、**抽象度の高い目標**として設計する（phase3_reward_discussion.pdfの解決策1に対応）：

- **成功報酬**: タスク完了時に +1.0
- **進捗報酬**: タスクに向けた有意義な進捗（例：容器に近づく、取っ手を掴む）に対して +0.1
- **失敗ペナルティ**: オブジェクトを落とす、倒すなどに対して -0.1
- **タイムステップペナルティ**: -0.01（効率的な行動を促す）

重要な点：報酬は「どのように達成するか」を指定せず、「何を達成するか」のみを指定する。

### 2.4. Agent Cのアーキテクチャ

#### 2.4.1. 状態表現

Agent Cの状態 `s_t` は以下から構成される：

1. **アフォーダンス特徴**: `F(x_t)` （Phase 1で訓練済みのFunctor Fの出力）
2. **ロボット状態**: 関節角度、エンドエフェクタ位置、グリッパー状態
3. **LSTM隠れ状態**: `h_{t-1}` （前タイムステップの注意履歴を保持）

#### 2.4.2. ネットワーク構造

```
入力: [F(x_t), robot_state_t, h_{t-1}]
  ↓
LSTM層 → h_t （隠れ状態更新）
  ↓
Actor分岐:
  - 注意選択ヘッド → π_attention(s_t) （離散行動、Softmax）
  - 物理行動ヘッド → π_physical(s_t) （連続行動、Tanh）
  ↓
Critic分岐:
  - 価値関数ヘッド → V(s_t) または Q(s_t, a_t)
  - 軌跡予測ヘッド → η_trajectory(s_t, a_t) （N次元ベクトル、将来Nステップのη予測）
```

#### 2.4.3. 学習アルゴリズム

- **Actor-Critic with LSTM**: PPO（Proximal Policy Optimization）またはSAC（Soft Actor-Critic）
- **軌跡予測の学習**: 予測したη軌跡と実際のη軌跡の平均二乗誤差で学習
- **報酬の二重構造**: `r(t) = α · r_intrinsic(t) + β · r_task(t)`
  - 初期値：α=1.0, β=0.5（実験的に調整）

### 2.5. 訓練プロトコル

#### 2.5.1. エピソード構造

- **エピソード長**: 最大100ステップ
- **終了条件**:
  1. タスク成功
  2. タスク失敗（オブジェクトが落下など）
  3. 最大ステップ数到達

#### 2.5.2. 訓練スケジュール

1. **Phase 2.1a (1000エピソード)**: Container Fillingタスクで訓練
2. **Phase 2.1b (1000エピソード)**: Articulated Object Manipulationタスクで訓練
3. **Phase 2.1c (500エピソード)**: 未知のオブジェクトでテスト（汎化性能の検証）

#### 2.5.3. チェックポイントとモニタリング

- **チェックポイント**: 100エピソードごとに保存
- **進捗出力**: 10エピソードごとにログ出力（sys.stdout.flush()で確実に出力）
- **評価指標**:
  - タスク成功率
  - 累積報酬（内発的報酬と外発的報酬を分けて記録）
  - 軌跡予測誤差
  - 平均エピソード長

## 3. 理論的正当性の検証

この設計が、phase3_reward_discussion.pdfで提起された問題をどのように解決するかを示す。

### 3.1. 問題点1への対応：外発的報酬の使用

**問題**: タスク報酬（外発的報酬）を使うと「目的の創発」と矛盾する。

**解決策**: 報酬の二重構造 `r(t) = α · r_intrinsic(t) + β · r_task(t)`

- **内発的報酬** `r_intrinsic(t) = α · (η_whole - η_part)` は、エージェントの「理解への駆動力」を表現
- **タスク報酬** `r_task(t)` は、エージェントの「目的」を表現
- 両者のバランスを調整することで、「理解を深めながら目的を達成する」行動を学習
- 重要：タスク報酬は「何を達成するか」のみを指定し、「どのように達成するか」は指定しない（抽象度の高い報酬）

**理論的解釈**: 
- タスク報酬を用いることは、理論からの逸脱ではなく、「目的を通した随伴」を実装に落とし込むための正当なアプローチである（phase3_reward_discussion.pdf, セクション4.3）
- エージェントの目的を実装レベルで表現する上で、タスク報酬は最も直接的で正当な方法である

### 3.2. 問題点2への対応：F/Gとの接続の欠落

**問題**: PyBulletの点群をF/Gに通してηを計算し、Agent Cにフィードバックするパイプラインが不明確。

**解決策**: セクション2.2で明確な統合パイプラインを定義

1. 各タイムステップで点群をF/Gに通してηを計算
2. ηから内発的報酬を計算
3. F(x)のアフォーダンス特徴をAgent Cの状態表現に含める
4. Agent Cの行動が環境を変化させ、次のタイムステップの点群（とη）が変化
5. この循環を通じて、F/GとAgent Cの相互作用（随伴）が成立

**理論的解釈**:
- F/Gは「川床」として、ゆっくりと進化した世界モデルを提供（Phase 1で事前訓練済み）
- Agent Cは「水の流れ」として、価値関数に基づいて速い時間スケールで行動
- 随伴は、F/Gが提供するアフォーダンス特徴と、Agent Cの目的（タスク報酬）を通じて成立

### 3.3. 問題点3への対応：Pickingタスクの単純さ

**問題**: Pickingタスクは単一行動で完結するため、アフォーダンス推測能力の検証には不十分。

**解決策**: セクション2.3で複数のインタラクションが可能なタスクを設計

- **Container Filling**: 容器の開口部、深さ、安定性など、複数のアフォーダンスを理解する必要がある
- **Articulated Object Manipulation**: 取っ手の位置、引く/押す方向、関節の軸など、複数のインタラクション方法を試す必要がある
- これらのタスクでは、エージェントは「同じオブジェクトに対して複数の異なる関わり方が可能」な状況で、最適な行動を選択する必要がある

**理論的解釈**:
- アフォーダンスの真価が問われるのは、「同じオブジェクトに対して複数の異なる関わり方が可能」な状況である（phase3_reward_discussion.pdf, セクション3.2）
- 複雑なタスクを通じて、エージェントは真にアフォーダンスを推測する能力を獲得する

## 4. 実装計画

### 4.1. ディレクトリ構造

```
experiments/phase2.1_pybullet/
├── DESIGN_PROPOSAL_v2.2_PYBULLET.md  （本文書）
├── REQUIREMENTS.md  （実装要件の詳細）
├── env/
│   ├── pybullet_env.py  （PyBullet環境のラッパー）
│   ├── tasks.py  （タスク定義）
│   └── utils.py  （点群変換などのユーティリティ）
├── agent/
│   ├── agent_c_pybullet.py  （LSTM-based Actor-Critic Agent C）
│   └── replay_buffer.py  （経験リプレイバッファ）
├── train_phase2.1.py  （訓練スクリプト）
├── eval_phase2.1.py  （評価スクリプト）
├── checkpoints/  （チェックポイント保存先）
└── logs/  （訓練ログ）
```

### 4.2. 実装ステップ

#### Step 1: PyBullet環境の構築 (2-3日)

- [ ] PyBulletのインストールと基本シーンの構築
- [ ] カメラから点群を取得する機能の実装
- [ ] KUKAアームの読み込みとIK制御の実装
- [ ] Container Fillingタスクの実装

#### Step 2: F/G統合パイプラインの実装 (1-2日)

- [ ] Phase 1のチェックポイント（`phase1_final.pt`）をロード
- [ ] 点群をF/Gに通してηを計算する関数の実装
- [ ] 内発的報酬の計算関数の実装

#### Step 3: Agent Cの実装 (3-4日)

- [ ] LSTM-based Actor-Criticアーキテクチャの実装
- [ ] 注意選択ヘッドと物理行動ヘッドの実装
- [ ] 軌跡予測ヘッドの実装
- [ ] PPO/SACアルゴリズムの実装

#### Step 4: 訓練ループの実装 (2-3日)

- [ ] エピソード実行ループの実装
- [ ] チェックポイント保存・再開機能の実装
- [ ] 進捗モニタリングとログ出力の実装
- [ ] 評価指標の計算と記録

#### Step 5: 実験実行と分析 (5-7日)

- [ ] Phase 2.1a (Container Filling) の訓練実行
- [ ] Phase 2.1b (Articulated Object) の訓練実行
- [ ] Phase 2.1c (未知オブジェクト) のテスト実行
- [ ] 結果の分析とRESULTS.mdの作成

**総所要時間**: 約2-3週間

### 4.3. 依存パッケージ

```bash
sudo pip3 install pybullet
sudo pip3 install gym  # 環境ラッパー用
sudo pip3 install tensorboard  # ログ可視化用
```

## 5. 期待される成果

### 5.1. 定量的成果

- **タスク成功率**: 80%以上（1000エピソード訓練後）
- **軌跡予測誤差**: 訓練を通じて減少傾向を示す
- **汎化性能**: 未知オブジェクトに対して50%以上の成功率

### 5.2. 定性的成果

- **F/GとAgent Cの随伴の実証**: 物理シミュレーション環境において、F/Gが提供するアフォーダンス特徴がAgent Cの行動に意味のある影響を与えることを示す
- **報酬の二重構造の有効性**: 内発的報酬（η改善）とタスク報酬（目的達成）を両立させることで、「理解を深めながら目的を達成する」行動が学習されることを示す
- **軌跡予測の有効性**: 価値関数が軌跡を予測することで、より効率的な探索が可能になることを示す

### 5.3. 理論的貢献

- **「目的を通した随伴」の具体的実装**: phase3_reward_discussion.pdfで提起された理論的問題に対する、実装レベルでの解決策を提示
- **内発的報酬と外発的報酬の統合**: 両者を対立させるのではなく、補完的に機能させる設計パターンを示す
- **物理的グラウンディングによるアフォーダンス学習**: 抽象的な注意選択タスクから、物理的相互作用を伴うタスクへの拡張

## 6. 未解決の問い（今後の研究課題）

phase3_reward_discussion.pdfで提起された未解決の問いのうち、Phase 2.1で部分的に検証できるもの：

1. **内発的報酬の限界**: ηベースの内発的報酬だけで目的指向の行動が発見できるか？
   - Phase 2.1では、報酬の二重構造を用いることで、この問いを部分的に回避する
   - 将来的には、α=1.0, β=0.0（内発的報酬のみ）での実験も検討

2. **タスク報酬の適切な抽象度**: どのレベルの抽象度が最適か？
   - Phase 2.1では、「何を達成するか」のみを指定する抽象的な報酬を用いる
   - 異なる抽象度の報酬を比較する実験も今後検討

3. **物理シミュレーションにおけるηの役割**: エージェントの行動がオブジェクトの形状を変化させたとき、ηはどのように変化するか？
   - Phase 2.1で直接検証可能
   - ηの変化パターンがアフォーダンスに関する有益な情報を含むかを分析

## 7. まとめ

Phase 2.1 v2.2は、phase3_reward_discussion.pdfで提起された理論的課題を真摯に受け止め、以下の方針で設計された：

1. **PyBullet物理シミュレーション**によるアフォーダンスの物理的グラウンディング
2. **報酬の二重構造**による内発的報酬と外発的報酬の統合
3. **F/GとAgent Cの明確な統合パイプライン**による随伴の成立
4. **複数インタラクション可能なタスク**によるアフォーダンス推測能力の真の検証

この設計は、「目的を通した随伴」という理論的核心を、実装レベルで具体化する試みである。Phase 2.1の実験を通じて、プロジェクトは「内省的な観察者」から「世界と相互作用する実践者」へと進化する。
