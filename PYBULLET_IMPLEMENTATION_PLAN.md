## Phase 3 実装計画：PyBulletによる物理シミュレーション環境

PyBullet導入の妥当性に基づき、Phase 3として物理シミュレーション環境を構築する具体的な実装計画を提案します。

### 1. 全体アーキテクチャ

システムは以下の3つの主要コンポーネントで構成されます。

1.  **PyBullet Environment**: 物理シミュレーション環境。オブジェクトの配置、ロボットアームの制御、物理演算、状態観測（カメラ画像、点群）を担当。
2.  **Adjunction Model (F/G + Agent C)**: 観測された状態からアフォーダンスを推論し、行動を決定するエージェント。
3.  **Training Loop**: エージェントと環境の相互作用を管理し、強化学習（RL）アルゴリズムに基づいてエージェントを訓練するメインループ。

### 2. 実装ステップ

#### Step 1: 基本的なPyBullet環境の構築

- **担当**: `src/simulation/pybullet_env.py`
- **内容**:
  - PyBulletの初期化（GUIモードとDirectモード）
  - 基本的なオブジェクト（平面、テーブル、立方体）の読み込みと配置
  - カメラの設定と、RGB画像、深度画像、セグメンテーションマスクを取得する機能の実装
  - 取得した画像から点群を生成するユーティリティ関数

#### Step 2: ロボットアームの導入と制御

- **担当**: `src/simulation/robot_arm.py`
- **内容**:
  - KUKAアームなどの標準的なロボットアームモデルを読み込み
  - エンドエフェクタ（グリッパー）の制御（開閉）
  - 逆運動学（IK）を用いて、ターゲットの座標にアームを動かす`move_to(x, y, z)`関数の実装
  - `grasp()`と`release()`という高レベルな行動関数の実装

#### Step 3: タスクの定義と報酬設計

- **担当**: `src/simulation/tasks.py`
- **内容**:
  - まずは最も基本的な**「Picking」タスク**を実装
  - **初期状態**: テーブルの上にランダムな位置・角度でオブジェクトを配置
  - **ゴール**: オブジェクトを掴んで、一定の高さまで持ち上げる
  - **報酬関数**:
    - `+1.0`: タスク成功（オブジェクトを持ち上げた）
    - `-0.1`: アームがオブジェクトに触れたが、掴めなかった
    - `-0.01`: タイムステップごとのペナルティ（効率的な行動を促進）

#### Step 4: Agent Cの行動空間と状態空間の再設計

- **担当**: `src/models/agent_c_physical.py`
- **内容**:
  - **状態空間**: PyBulletから取得したカメラ画像（または点群）を直接入力とする。
  - **行動空間**: 連続的な行動空間とし、`[dx, dy, dz, d_roll, d_pitch, d_yaw, gripper_state]`（アームの移動量とグリッパーの状態）を予測する。
  - **アルゴリズム**: DQNは離散行動空間にしか対応できないため、**DDPG (Deep Deterministic Policy Gradient)**や**SAC (Soft Actor-Critic)**などの連続行動空間に対応したアルゴリズムを採用する。

#### Step 5: 訓練ループの実装

- **担当**: `experiments/phase3_pybullet_picking/run_phase3.py`
- **内容**:
  - エピソードベースの訓練ループを実装
  - 各ステップで、Agent Cが状態を観測し、行動を決定
  - PyBullet環境で行動を実行し、次の状態と報酬を受け取る
  - 経験（state, action, reward, next_state）をリプレイバッファに保存
  - リプレイバッファからサンプリングして、DDPG/SACアルゴリズムでAgent Cを更新

### 3. 優先順位とマイルストーン

1.  **M1: 静的なシーンのレンダリング**: PyBulletでオブジェクトを配置し、カメラから画像と点群を取得できる。（Step 1）
2.  **M2: アームの基本的な動作**: ロボットアームを読み込み、指定した座標に動かせる。（Step 2）
3.  **M3: Pickingタスクの実行**: エージェントがランダムに行動し、Pickingタスクが実行され、報酬が計算される。（Step 3, 4, 5）
4.  **M4: 学習の開始**: DDPG/SACによる学習が開始され、成功率がランダム以上になる。（Step 5）

この計画により、プロジェクトは物理世界との相互作用を通じて真の汎化能力を獲得するという、最も重要かつ挑戦的なフェーズへと移行します。
